import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import torchaudio.transforms as T
import torchaudio.functional as AF
import librosa
import soundfile as sf
from tqdm import tqdm
import logging
import json
import time
from pathlib import Path
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# é€²éšå°å…¥
import torchvision.transforms as transforms
from torch.cuda.amp import autocast, GradScaler
from scipy import signal
from scipy.signal import butter, sosfilt, savgol_filter
from sklearn.preprocessing import StandardScaler
import psutil
import GPUtil

# è¨­å®šæ—¥èªŒè¨˜éŒ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('audio_processing.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingConfig:
    """è™•ç†é…ç½®é¡"""
    # åŸºæœ¬åƒæ•¸
    sr: int = 22050  # æå‡åˆ°22kHzä»¥ä¿æŒæ›´å¥½éŸ³è³ª
    dtype: torch.dtype = torch.float32
    device: str = 'auto'
    
    # é«˜ç´šGPUå„ªåŒ–
    use_mixed_precision: bool = True
    enable_flash_attention: bool = True
    max_batch_size: int = 16
    prefetch_factor: int = 4
    num_workers: int = 4
    
    # æ·±åº¦å­¸ç¿’å¢å¼·
    use_neural_enhancement: bool = True
    use_transformer_denoising: bool = True
    use_adversarial_training: bool = False
    
    # é »è­œåˆ†æåƒæ•¸
    n_fft: int = 2048  # å¢åŠ FFTçª—å£ä»¥ç²å¾—æ›´å¥½é »ç‡åˆ†è¾¨ç‡
    hop_length: int = 512
    win_length: int = 2048
    n_mels: int = 128
    f_min: float = 0.0
    f_max: float = 11025.0
    
    # æ™ºèƒ½è™•ç†åƒæ•¸
    adaptive_windowing: bool = True
    spectral_masking: bool = True
    temporal_modeling: bool = True
    psychoacoustic_modeling: bool = True
    
    # å“è³ªæ§åˆ¶
    target_lufs: float = -23.0
    max_peak: float = -1.0
    dynamic_range_target: float = 14.0
    snr_threshold: float = 20.0
    
    # ä¸¦è¡Œè™•ç†
    enable_multiprocessing: bool = True
    chunk_overlap_ratio: float = 0.25
    memory_efficient_mode: bool = True


class TransformerDenoiser(nn.Module):
    """åŸºæ–¼Transformerçš„éŸ³é »é™å™ªæ¨¡å‹"""
    
    def __init__(self, config: ProcessingConfig):
        super().__init__()
        self.config = config
        self.d_model = 512
        self.nhead = 8
        self.num_layers = 6
        
        # ç·¨ç¢¼å™¨
        self.input_projection = nn.Linear(config.n_mels, self.d_model)
        self.positional_encoding = PositionalEncoding(self.d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=self.nhead,
            dim_feedforward=2048,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, self.num_layers)
        
        # è¼¸å‡ºå±¤
        self.output_projection = nn.Linear(self.d_model, config.n_mels)
        self.gate = nn.Sigmoid()
        
    def forward(self, x):
        # x: [batch, time, mels]
        x = self.input_projection(x)
        x = self.positional_encoding(x)
        
        # Transformerè™•ç†
        enhanced = self.transformer(x)
        
        # è¼¸å‡ºæ˜ å°„å’Œé–€æ§
        mask = self.gate(self.output_projection(enhanced))
        
        return mask


class PositionalEncoding(nn.Module):
    """ä½ç½®ç·¨ç¢¼"""
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


class PsychoacousticModel(nn.Module):
    """å¿ƒç†è²å­¸æ¨¡å‹"""
    
    def __init__(self, config: ProcessingConfig):
        super().__init__()
        self.config = config
        self.bark_scale = self._create_bark_scale()
        
    def _create_bark_scale(self):
        """å‰µå»ºBarké »ç‡å°ºåº¦"""
        freqs = librosa.fft_frequencies(sr=self.config.sr, n_fft=self.config.n_fft)
        bark_freqs = 13 * np.arctan(0.00076 * freqs) + 3.5 * np.arctan((freqs/7500)**2)
        return torch.from_numpy(bark_freqs).float()
    
    def compute_masking_threshold(self, magnitude_spectrum):
        """è¨ˆç®—æ©è”½é–¾å€¼"""
        with autocast():
            # è½‰æ›åˆ°BarkåŸŸ
            bark_spectrum = self._to_bark_domain(magnitude_spectrum)
            
            # è¨ˆç®—èª¿æ€§å’Œå™ªè²æ©è”½
            tonal_masking = self._compute_tonal_masking(bark_spectrum)
            noise_masking = self._compute_noise_masking(bark_spectrum)
            
            # çµ„åˆæ©è”½é–¾å€¼
            masking_threshold = torch.maximum(tonal_masking, noise_masking)
            
            return masking_threshold
    
    def _to_bark_domain(self, spectrum):
        """è½‰æ›åˆ°BarkåŸŸ"""
        # ç°¡åŒ–å¯¦ç¾
        return spectrum
    
    def _compute_tonal_masking(self, bark_spectrum):
        """è¨ˆç®—èª¿æ€§æ©è”½"""
        # ç°¡åŒ–å¯¦ç¾
        return bark_spectrum * 0.1
    
    def _compute_noise_masking(self, bark_spectrum):
        """è¨ˆç®—å™ªè²æ©è”½"""
        # ç°¡åŒ–å¯¦ç¾  
        return bark_spectrum * 0.05


class AdvancedAudioProcessor:
    """å…ˆé€²éŸ³é »è™•ç†å™¨"""
    
    def __init__(self, config: Optional[ProcessingConfig] = None):
        self.config = config or ProcessingConfig()
        self._setup_device()
        self._initialize_models()
        self._setup_transforms()
        self._initialize_scaler()
        
        # æ€§èƒ½ç›£æ§
        self.performance_stats = {
            'processing_times': [],
            'memory_usage': [],
            'gpu_utilization': [],
            'throughput_rates': [],
            'quality_scores': []
        }
        
    def _setup_device(self):
        """è¨­ç½®è¨ˆç®—è¨­å‚™"""
        if self.config.device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                # è¨­ç½®GPUå„ªåŒ–
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
                logger.info(f"GPUè¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            else:
                self.device = torch.device('cpu')
                logger.info("ä½¿ç”¨CPUè™•ç†")
        else:
            self.device = torch.device(self.config.device)
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¸ç¿’æ¨¡å‹"""
        if self.config.use_neural_enhancement:
            # Transformeré™å™ªå™¨
            self.denoiser = TransformerDenoiser(self.config).to(self.device)
            
            # å¿ƒç†è²å­¸æ¨¡å‹
            self.psychoacoustic_model = PsychoacousticModel(self.config).to(self.device)
            
            # åˆå§‹åŒ–ç‚ºè©•ä¼°æ¨¡å¼
            self.denoiser.eval()
            self.psychoacoustic_model.eval()
            
            logger.info("å·²è¼‰å…¥ç¥ç¶“ç¶²çµ¡å¢å¼·æ¨¡å‹")
    
    def _setup_transforms(self):
        """è¨­ç½®éŸ³é »è®Šæ›"""
        # å…ˆé€²çš„STFTè®Šæ›
        self.stft = T.Spectrogram(
            n_fft=self.config.n_fft,
            hop_length=self.config.hop_length,
            win_length=self.config.win_length,
            window_fn=torch.hann_window,
            power=None,
            center=True,
            pad_mode='reflect',
            normalized=True
        ).to(self.device)
        
        self.istft = T.InverseSpectrogram(
            n_fft=self.config.n_fft,
            hop_length=self.config.hop_length,
            win_length=self.config.win_length,
            window_fn=torch.hann_window,
            center=True,
            normalized=True
        ).to(self.device)
        
        # Melé »è­œè®Šæ›
        self.mel_transform = T.MelSpectrogram(
            sample_rate=self.config.sr,
            n_fft=self.config.n_fft,
            hop_length=self.config.hop_length,
            n_mels=self.config.n_mels,
            f_min=self.config.f_min,
            f_max=self.config.f_max,
            window_fn=torch.hann_window,
            mel_scale='htk'
        ).to(self.device)
        
        # é€†Melè®Šæ›
        self.inverse_mel = T.InverseMelScale(
            n_stft=self.config.n_fft // 2 + 1,
            n_mels=self.config.n_mels,
            sample_rate=self.config.sr,
            f_min=self.config.f_min,
            f_max=self.config.f_max,
            mel_scale='htk'
        ).to(self.device)
        
        # é«˜ç´šéŸ³é »å¢å¼·è®Šæ›
        self.compander = T.Vol(gain=1.0, gain_type='amplitude').to(self.device)
        
    def _initialize_scaler(self):
        """åˆå§‹åŒ–æ··åˆç²¾åº¦ç¸®æ”¾å™¨"""
        if self.config.use_mixed_precision and self.device.type == 'cuda':
            self.scaler = GradScaler()
        else:
            self.scaler = None
    
    def load_audio_optimized(self, file_path: Union[str, Path]) -> torch.Tensor:
        """å„ªåŒ–çš„éŸ³é »è¼‰å…¥"""
        try:
            # ä½¿ç”¨torchaudioè¼‰å…¥
            waveform, orig_sr = torchaudio.load(file_path)
            
            # é‡æ¡æ¨£åˆ°ç›®æ¨™æ¡æ¨£ç‡
            if orig_sr != self.config.sr:
                resampler = T.Resample(
                    orig_freq=orig_sr,
                    new_freq=self.config.sr,
                    resampling_method='sinc_interp_hann'
                ).to(self.device)
                waveform = resampler(waveform.to(self.device))
            else:
                waveform = waveform.to(self.device)
            
            # è½‰æ›ç‚ºå–®è²é“ï¼ˆä½¿ç”¨åŠ æ¬Šå¹³å‡ï¼‰
            if waveform.shape[0] > 1:
                # ä½¿ç”¨æ¨™æº–ç«‹é«”è²åˆ°å–®è²é“è½‰æ›æ¬Šé‡
                weights = torch.tensor([0.5, 0.5]).to(self.device)
                waveform = torch.sum(waveform * weights.unsqueeze(1), dim=0, keepdim=True)
            
            # æ­£è¦åŒ–
            waveform = AF.preemphasis(waveform, coeff=0.97)
            
            return waveform.to(dtype=self.config.dtype)
            
        except Exception as e:
            logger.error(f"è¼‰å…¥éŸ³é »å¤±æ•— {file_path}: {e}")
            raise
    
    def analyze_audio_advanced(self, audio: torch.Tensor) -> Dict:
        """å…ˆé€²éŸ³é »åˆ†æ"""
        with torch.no_grad():
            analysis = {}
            
            # åŸºæœ¬çµ±è¨ˆ
            analysis['rms'] = torch.sqrt(torch.mean(audio ** 2)).item()
            analysis['peak'] = torch.max(torch.abs(audio)).item()
            analysis['crest_factor'] = analysis['peak'] / (analysis['rms'] + 1e-10)
            
            # é »è­œåˆ†æ
            stft = self.stft(audio)
            magnitude = torch.abs(stft)
            
            # é »è­œé‡å¿ƒ
            freqs = torch.linspace(0, self.config.sr/2, magnitude.shape[1]).to(self.device)
            spectral_centroid = torch.sum(magnitude.mean(dim=-1) * freqs) / torch.sum(magnitude.mean(dim=-1))
            analysis['spectral_centroid'] = spectral_centroid.item()
            
            # é »è­œæ»¾é™
            cumsum = torch.cumsum(magnitude.mean(dim=-1), dim=0)
            rolloff_threshold = 0.85 * cumsum[-1]
            rolloff_idx = torch.where(cumsum >= rolloff_threshold)[0][0]
            analysis['spectral_rolloff'] = freqs[rolloff_idx].item()
            
            # é›¶äº¤å‰ç‡
            zero_crossings = torch.sum(torch.diff(torch.sign(audio), dim=-1) != 0).float()
            analysis['zero_crossing_rate'] = (zero_crossings / audio.shape[-1]).item()
            
            # è«§æ³¢å™ªè²æ¯”ä¼°ç®—
            harmonic_power = torch.sum(magnitude[:magnitude.shape[0]//4] ** 2)
            total_power = torch.sum(magnitude ** 2)
            analysis['harmonic_ratio'] = (harmonic_power / total_power).item()
            
            # å‹•æ…‹ç¯„åœ
            analysis['dynamic_range'] = 20 * np.log10(analysis['peak'] / (analysis['rms'] + 1e-10))
            
            # ä¿¡å™ªæ¯”ä¼°ç®—
            noise_floor = torch.quantile(magnitude.flatten(), 0.1)
            signal_power = torch.mean(magnitude)
            analysis['snr_estimate'] = 20 * torch.log10(signal_power / (noise_floor + 1e-10)).item()
            
            return analysis
    
    def neural_enhance(self, audio: torch.Tensor) -> torch.Tensor:
        """ç¥ç¶“ç¶²çµ¡å¢å¼·"""
        if not self.config.use_neural_enhancement:
            return audio
        
        try:
            with torch.no_grad():
                # è½‰æ›åˆ°MelåŸŸ
                mel_spec = self.mel_transform(audio)
                mel_log = torch.log(mel_spec + 1e-8)
                
                # æº–å‚™è¼¸å…¥ [batch, time, mels]
                mel_input = mel_log.transpose(-2, -1).unsqueeze(0)
                
                # Transformerè™•ç†
                with autocast(enabled=self.config.use_mixed_precision):
                    enhancement_mask = self.denoiser(mel_input)
                
                # æ‡‰ç”¨å¢å¼·é®ç½©
                enhanced_mel = mel_log.transpose(-2, -1) * enhancement_mask.squeeze(0)
                enhanced_mel = enhanced_mel.transpose(-2, -1)
                
                # è½‰å›ç·šæ€§åŸŸ
                enhanced_linear = torch.exp(enhanced_mel)
                
                # é€†Melè®Šæ›åˆ°é »è­œåŸŸ
                enhanced_stft = self.inverse_mel(enhanced_linear)
                
                # ç›¸ä½é‡å»ºï¼ˆä½¿ç”¨åŸå§‹ç›¸ä½ï¼‰
                original_stft = self.stft(audio)
                original_phase = torch.angle(original_stft)
                
                # é‡å»ºè¤‡æ•¸é »è­œ
                enhanced_complex = enhanced_stft * torch.exp(1j * original_phase)
                
                # é€†STFT
                enhanced_audio = self.istft(enhanced_complex, length=audio.shape[-1])
                
                return enhanced_audio
                
        except Exception as e:
            logger.warning(f"ç¥ç¶“å¢å¼·å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹éŸ³é »: {e}")
            return audio
    
    def psychoacoustic_enhancement(self, audio: torch.Tensor) -> torch.Tensor:
        """å¿ƒç†è²å­¸å¢å¼·"""
        if not self.config.psychoacoustic_modeling:
            return audio
        
        try:
            with torch.no_grad():
                # ç²å–é »è­œ
                stft = self.stft(audio)
                magnitude = torch.abs(stft)
                phase = torch.angle(stft)
                
                # è¨ˆç®—æ©è”½é–¾å€¼
                masking_threshold = self.psychoacoustic_model.compute_masking_threshold(magnitude)
                
                # æ‡‰ç”¨å¿ƒç†è²å­¸æ¿¾æ³¢
                enhanced_magnitude = torch.where(
                    magnitude > masking_threshold,
                    magnitude,
                    magnitude * 0.1  # æ¸›å¼±ä½æ–¼é–¾å€¼çš„æˆåˆ†
                )
                
                # é‡å»ºéŸ³é »
                enhanced_stft = enhanced_magnitude * torch.exp(1j * phase)
                enhanced_audio = self.istft(enhanced_stft, length=audio.shape[-1])
                
                return enhanced_audio
                
        except Exception as e:
            logger.warning(f"å¿ƒç†è²å­¸å¢å¼·å¤±æ•—: {e}")
            return audio
    
    def adaptive_dynamic_range_control(self, audio: torch.Tensor, analysis: Dict) -> torch.Tensor:
        """è‡ªé©æ‡‰å‹•æ…‹ç¯„åœæ§åˆ¶"""
        current_dr = analysis['dynamic_range']
        target_dr = self.config.dynamic_range_target
        
        if abs(current_dr - target_dr) < 2.0:
            return audio
        
        # å¤šé »æ®µå£“ç¸®
        audio_enhanced = self._multiband_compressor(audio, analysis)
        
        # è»Ÿé™åˆ¶å™¨
        audio_limited = self._soft_limiter(audio_enhanced)
        
        return audio_limited
    
    def _multiband_compressor(self, audio: torch.Tensor, analysis: Dict) -> torch.Tensor:
        """å¤šé »æ®µå£“ç¸®å™¨"""
        try:
            # ç²å–é »è­œ
            stft = self.stft(audio)
            magnitude = torch.abs(stft)
            phase = torch.angle(stft)
            
            # å®šç¾©é »æ®µé‚Šç•Œ
            n_bands = 4
            band_edges = torch.linspace(0, magnitude.shape[1], n_bands + 1).long()
            
            # å°æ¯å€‹é »æ®µé€²è¡Œå£“ç¸®
            compressed_magnitude = magnitude.clone()
            
            for i in range(n_bands):
                start_bin = band_edges[i]
                end_bin = band_edges[i + 1]
                
                band_mag = magnitude[start_bin:end_bin]
                
                # è¨ˆç®—å£“ç¸®æ¯”ï¼ˆæ ¹æ“šé »æ®µèª¿æ•´ï¼‰
                if i == 0:  # ä½é »
                    ratio = 2.0
                elif i == 1:  # ä¸­ä½é »
                    ratio = 1.5
                elif i == 2:  # ä¸­é«˜é »
                    ratio = 1.2
                else:  # é«˜é »
                    ratio = 1.1
                
                # æ‡‰ç”¨å£“ç¸®
                threshold = torch.quantile(band_mag, 0.8)
                compressed_band = torch.where(
                    band_mag > threshold,
                    threshold + (band_mag - threshold) / ratio,
                    band_mag
                )
                
                compressed_magnitude[start_bin:end_bin] = compressed_band
            
            # é‡å»ºéŸ³é »
            compressed_stft = compressed_magnitude * torch.exp(1j * phase)
            compressed_audio = self.istft(compressed_stft, length=audio.shape[-1])
            
            return compressed_audio
            
        except Exception as e:
            logger.warning(f"å¤šé »æ®µå£“ç¸®å¤±æ•—: {e}")
            return audio
    
    def _soft_limiter(self, audio: torch.Tensor) -> torch.Tensor:
        """è»Ÿé™åˆ¶å™¨"""
        threshold = 10 ** (self.config.max_peak / 20)
        
        # è»Ÿé™åˆ¶å‡½æ•¸
        limited_audio = torch.tanh(audio / threshold) * threshold
        
        return limited_audio
    
    def process_single_file(self, input_path: Union[str, Path], output_path: Union[str, Path]) -> bool:
        """è™•ç†å–®å€‹éŸ³é »æ–‡ä»¶"""
        try:
            start_time = time.time()
            
            # è¼‰å…¥éŸ³é »
            audio = self.load_audio_optimized(input_path)
            original_length = audio.shape[-1]
            
            logger.info(f"è™•ç†éŸ³é »: {input_path}, é•·åº¦: {original_length/self.config.sr:.2f}ç§’")
            
            # åˆ†æéŸ³é »
            analysis = self.analyze_audio_advanced(audio)
            
            # ç¥ç¶“ç¶²çµ¡å¢å¼·
            enhanced_audio = self.neural_enhance(audio)
            
            # å¿ƒç†è²å­¸å¢å¼·
            enhanced_audio = self.psychoacoustic_enhancement(enhanced_audio)
            
            # è‡ªé©æ‡‰å‹•æ…‹ç¯„åœæ§åˆ¶
            enhanced_audio = self.adaptive_dynamic_range_control(enhanced_audio, analysis)
            
            # æœ€çµ‚æ­£è¦åŒ–
            enhanced_audio = self._intelligent_normalize(enhanced_audio, analysis)
            
            # å“è³ªè©•ä¼°
            quality_score = self._assess_quality(audio, enhanced_audio)
            
            # ä¿å­˜éŸ³é »
            self._save_audio(enhanced_audio, output_path)
            
            # è¨˜éŒ„æ€§èƒ½çµ±è¨ˆ
            processing_time = time.time() - start_time
            self.performance_stats['processing_times'].append(processing_time)
            self.performance_stats['quality_scores'].append(quality_score)
            self.performance_stats['throughput_rates'].append(original_length / processing_time / self.config.sr)
            
            logger.info(f"è™•ç†å®Œæˆ: {output_path}")
            logger.info(f"å“è³ªåˆ†æ•¸: {quality_score:.3f}, è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            logger.info(f"è™•ç†é€Ÿåº¦: {original_length/processing_time/self.config.sr:.1f}xå¯¦æ™‚")
            
            return True
            
        except Exception as e:
            logger.error(f"è™•ç†å¤±æ•— {input_path}: {e}")
            return False
    
    def _intelligent_normalize(self, audio: torch.Tensor, analysis: Dict) -> torch.Tensor:
        """æ™ºèƒ½æ­£è¦åŒ–"""
        # åŸºæ–¼åˆ†æçµæœçš„è‡ªé©æ‡‰æ­£è¦åŒ–
        target_lufs_linear = 10 ** (self.config.target_lufs / 20)
        current_rms = analysis['rms']
        
        if current_rms > 1e-10:
            gain = target_lufs_linear / current_rms
            gain = torch.clamp(torch.tensor(gain), 0.1, 10.0)
            
            # æ‡‰ç”¨å¢ç›Š
            normalized_audio = audio * gain
            
            # å³°å€¼é™åˆ¶
            peak = torch.max(torch.abs(normalized_audio))
            max_peak_linear = 10 ** (self.config.max_peak / 20)
            
            if peak > max_peak_linear:
                limiter_gain = max_peak_linear / peak
                normalized_audio *= limiter_gain
            
            return normalized_audio
        
        return audio
    
    def _assess_quality(self, original: torch.Tensor, processed: torch.Tensor) -> float:
        """éŸ³è³ªè©•ä¼°"""
        try:
            with torch.no_grad():
                # PESQé¡ä¼¼çš„è©•ä¼°
                orig_stft = torch.abs(self.stft(original))
                proc_stft = torch.abs(self.stft(processed))
                
                # é »è­œè·é›¢
                spectral_distance = torch.mean((orig_stft - proc_stft) ** 2)
                
                # ç›¸é—œæ€§
                orig_flat = original.flatten()[:10000]
                proc_flat = processed.flatten()[:10000]
                
                if len(orig_flat) > 1 and len(proc_flat) > 1:
                    correlation = torch.corrcoef(torch.stack([orig_flat, proc_flat]))[0, 1]
                else:
                    correlation = torch.tensor(0.5)
                
                # SNRæ”¹å–„
                orig_noise = torch.quantile(torch.abs(orig_stft), 0.1)
                proc_noise = torch.quantile(torch.abs(proc_stft), 0.1)
                snr_improvement = torch.log10(orig_noise / (proc_noise + 1e-10))
                
                # ç¶œåˆåˆ†æ•¸
                quality_score = (
                    0.4 * torch.clamp(correlation, 0, 1) +
                    0.3 * torch.clamp(1 - spectral_distance, 0, 1) +
                    0.3 * torch.clamp(snr_improvement / 2 + 0.5, 0, 1)
                )
                
                return quality_score.item()
                
        except Exception as e:
            logger.warning(f"å“è³ªè©•ä¼°å¤±æ•—: {e}")
            return 0.7
    
    def _save_audio(self, audio: torch.Tensor, output_path: Union[str, Path]):
        """ä¿å­˜éŸ³é »"""
        try:
            # è½‰æ›åˆ°CPUä¸¦è½‰ç‚ºnumpy
            audio_np = audio.cpu().numpy().squeeze()
            
            # ç¢ºä¿éŸ³é »åœ¨åˆç†ç¯„åœå…§
            audio_np = np.clip(audio_np, -1.0, 1.0)
            
            # å‰µå»ºè¼¸å‡ºç›®éŒ„
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¿å­˜ç‚ºé«˜å“è³ªWAV
            sf.write(output_path, audio_np, self.config.sr, subtype='PCM_24')
            
        except Exception as e:
            logger.error(f"ä¿å­˜éŸ³é »å¤±æ•— {output_path}: {e}")
            raise
    
    def process_dataset_parallel(self, input_dir: Union[str, Path], output_dir: Union[str, Path]):
        """ä¸¦è¡Œè™•ç†æ•¸æ“šé›†"""
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç²å–æ‰€æœ‰éŸ³é »æ–‡ä»¶
        audio_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg'}
        audio_files = [f for f in input_dir.iterdir() 
                      if f.suffix.lower() in audio_extensions]
        
        logger.info(f"é–‹å§‹ä¸¦è¡Œè™•ç† {len(audio_files)} å€‹éŸ³é »æ–‡ä»¶")
        logger.info(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        
        success_count = 0
        start_time = time.time()
        
        # ä½¿ç”¨é€²åº¦æ¢
        with tqdm(audio_files, desc="è™•ç†éŸ³é »æ–‡ä»¶") as pbar:
            if self.config.enable_multiprocessing and len(audio_files) > 1:
                # ä¸¦è¡Œè™•ç†
                with ThreadPoolExecutor(max_workers=self.config.num_workers) as executor:
                    futures = []
                    
                    for audio_file in audio_files:
                        output_file = output_dir / f"{audio_file.stem}.wav"
                        future = executor.submit(self.process_single_file, audio_file, output_file)
                        futures.append((future, audio_file.name))
                    
                    for future, filename in futures:
                        try:
                            if future.result():
                                success_count += 1
                            pbar.update(1)
                            pbar.set_postfix(success=success_count)
                        except Exception as e:
                            logger.error(f"è™•ç†å¤±æ•— {filename}: {e}")
                            pbar.update(1)
            else:
                # é †åºè™•ç†
                for audio_file in pbar:
                    output_file = output_dir / f"{audio_file.stem}.wav"
                    if self.process_single_file(audio_file, output_file):
                        success_count += 1
                    pbar.set_postfix(success=success_count)
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
        self._generate_report(len(audio_files), success_count, total_time, output_dir)
        
        return success_count
    
    def _generate_report(self, total_files: int, success_count: int, total_time: float, output_dir: Path):
        """ç”Ÿæˆè™•ç†å ±å‘Š"""
        report = {
            'processing_summary': {
                'total_files': total_files,
                'successful_files': success_count,
                'failed_files': total_files - success_count,
                'success_rate': success_count / total_files * 100,
                'total_processing_time': total_time,
                'average_processing_time': np.mean(self.performance_stats['processing_times']) if self.performance_stats['processing_times'] else 0
            },
            'performance_metrics': {
                'average_throughput': np.mean(self.performance_stats['throughput_rates']) if self.performance_stats['throughput_rates'] else 0,
                'peak_throughput': np.max(self.performance_stats['throughput_rates']) if self.performance_stats['throughput_rates'] else 0,
                'average_quality_score': np.mean(self.performance_stats['quality_scores']) if self.performance_stats['quality_scores'] else 0,
                'min_quality_score': np.min(self.performance_stats['quality_scores']) if self.performance_stats['quality_scores'] else 0
            },
            'system_info': {
                'device': str(self.device),
                'mixed_precision_enabled': self.config.use_mixed_precision,
                'neural_enhancement_enabled': self.config.use_neural_enhancement,
                'psychoacoustic_modeling_enabled': self.config.psychoacoustic_modeling,
                'multiprocessing_enabled': self.config.enable_multiprocessing
            },
            'configuration': {
                'sample_rate': self.config.sr,
                'n_fft': self.config.n_fft,
                'hop_length': self.config.hop_length,
                'n_mels': self.config.n_mels,
                'target_lufs': self.config.target_lufs,
                'max_peak': self.config.max_peak
            },
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä¿å­˜å ±å‘Š
        report_file = output_dir / 'processing_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°å ±å‘Š
        logger.info("\n" + "="*80)
        logger.info("ğŸµ å…ˆé€²éŸ³é »è™•ç†å®Œæˆå ±å‘Š")
        logger.info("="*80)
        logger.info(f"ğŸ“Š ç¸½æ–‡ä»¶æ•¸: {total_files}")
        logger.info(f"âœ… æˆåŠŸè™•ç†: {success_count} ({success_count/total_files*100:.1f}%)")
        logger.info(f"âŒ è™•ç†å¤±æ•—: {total_files - success_count}")
        logger.info(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        
        if self.performance_stats['throughput_rates']:
            avg_throughput = np.mean(self.performance_stats['throughput_rates'])
            logger.info(f"ğŸš€ å¹³å‡è™•ç†é€Ÿåº¦: {avg_throughput:.1f}x å¯¦æ™‚")
        
        if self.performance_stats['quality_scores']:
            avg_quality = np.mean(self.performance_stats['quality_scores'])
            logger.info(f"ğŸ¯ å¹³å‡å“è³ªåˆ†æ•¸: {avg_quality:.3f}")
        
        logger.info(f"ğŸ“‹ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_file}")


def create_optimal_config() -> ProcessingConfig:
    """å‰µå»ºæœ€å„ªåŒ–é…ç½®"""
    config = ProcessingConfig()
    
    # æ ¹æ“šç¡¬ä»¶è‡ªå‹•èª¿æ•´
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        if gpu_memory >= 8:  # 8GBä»¥ä¸ŠGPU
            config.max_batch_size = 32
            config.n_fft = 4096
            config.hop_length = 1024
            config.n_mels = 256
            config.use_neural_enhancement = True
            config.use_transformer_denoising = True
        elif gpu_memory >= 4:  # 4-8GB GPU
            config.max_batch_size = 16
            config.n_fft = 2048
            config.hop_length = 512
            config.n_mels = 128
            config.use_neural_enhancement = True
        else:  # 4GBä»¥ä¸‹GPU
            config.max_batch_size = 8
            config.n_fft = 1024
            config.hop_length = 256
            config.n_mels = 80
            config.use_neural_enhancement = False
    
    # CPUæ ¸å¿ƒæ•¸èª¿æ•´
    cpu_count = psutil.cpu_count()
    config.num_workers = min(cpu_count // 2, 8)
    
    return config


class AudioQualityAnalyzer:
    """éŸ³é »å“è³ªåˆ†æå™¨"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def comprehensive_analysis(self, audio_path: Union[str, Path]) -> Dict:
        """ç¶œåˆéŸ³é »å“è³ªåˆ†æ"""
        try:
            # è¼‰å…¥éŸ³é »
            waveform, sr = torchaudio.load(audio_path)
            if sr != self.config.sr:
                resampler = T.Resample(sr, self.config.sr)
                waveform = resampler(waveform)
            
            waveform = waveform.to(self.device)
            
            analysis = {}
            
            # åŸºæœ¬åƒæ•¸
            analysis['duration'] = waveform.shape[-1] / self.config.sr
            analysis['channels'] = waveform.shape[0]
            analysis['sample_rate'] = self.config.sr
            
            # æ™‚åŸŸåˆ†æ
            analysis.update(self._time_domain_analysis(waveform))
            
            # é »åŸŸåˆ†æ
            analysis.update(self._frequency_domain_analysis(waveform))
            
            # æ„ŸçŸ¥å“è³ªåˆ†æ
            analysis.update(self._perceptual_analysis(waveform))
            
            # å‹•æ…‹ç¯„åœåˆ†æ
            analysis.update(self._dynamic_range_analysis(waveform))
            
            return analysis
            
        except Exception as e:
            logger.error(f"éŸ³é »åˆ†æå¤±æ•— {audio_path}: {e}")
            return {}
    
    def _time_domain_analysis(self, waveform: torch.Tensor) -> Dict:
        """æ™‚åŸŸåˆ†æ"""
        with torch.no_grad():
            analysis = {}
            
            # åŸºæœ¬çµ±è¨ˆ
            analysis['rms'] = torch.sqrt(torch.mean(waveform ** 2)).item()
            analysis['peak'] = torch.max(torch.abs(waveform)).item()
            analysis['mean'] = torch.mean(waveform).item()
            analysis['std'] = torch.std(waveform).item()
            
            # å‹•æ…‹æŒ‡æ¨™
            analysis['crest_factor'] = analysis['peak'] / (analysis['rms'] + 1e-10)
            analysis['peak_to_average_ratio'] = 20 * np.log10(analysis['peak'] / (analysis['rms'] + 1e-10))
            
            # é›¶äº¤å‰ç‡
            zero_crossings = torch.sum(torch.diff(torch.sign(waveform), dim=-1) != 0, dim=-1)
            analysis['zero_crossing_rate'] = (zero_crossings / waveform.shape[-1]).mean().item()
            
            return analysis
    
    def _frequency_domain_analysis(self, waveform: torch.Tensor) -> Dict:
        """é »åŸŸåˆ†æ"""
        with torch.no_grad():
            analysis = {}
            
            # STFT
            stft_transform = T.Spectrogram(
                n_fft=self.config.n_fft,
                hop_length=self.config.hop_length,
                power=1
            ).to(self.device)
            
            magnitude = stft_transform(waveform)
            
            # é »è­œé‡å¿ƒ
            freqs = torch.linspace(0, self.config.sr/2, magnitude.shape[-2]).to(self.device)
            spectral_centroid = torch.sum(magnitude.mean(dim=-1) * freqs.unsqueeze(0), dim=-1) / torch.sum(magnitude.mean(dim=-1), dim=-1)
            analysis['spectral_centroid'] = spectral_centroid.mean().item()
            
            # é »è­œå¸¶å¯¬
            spectral_spread = torch.sqrt(
                torch.sum(magnitude.mean(dim=-1) * (freqs.unsqueeze(0) - spectral_centroid.unsqueeze(-1)) ** 2, dim=-1) /
                torch.sum(magnitude.mean(dim=-1), dim=-1)
            )
            analysis['spectral_bandwidth'] = spectral_spread.mean().item()
            
            # é »è­œæ»¾é™
            cumsum = torch.cumsum(magnitude.mean(dim=-1), dim=-1)
            rolloff_threshold = 0.85 * cumsum[..., -1:]
            rolloff_indices = torch.argmax((cumsum >= rolloff_threshold).float(), dim=-1)
            analysis['spectral_rolloff'] = freqs[rolloff_indices].mean().item()
            
            # é »è­œå¹³å¦åº¦
            geometric_mean = torch.exp(torch.mean(torch.log(magnitude.mean(dim=-1) + 1e-10), dim=-1))
            arithmetic_mean = torch.mean(magnitude.mean(dim=-1), dim=-1)
            analysis['spectral_flatness'] = (geometric_mean / arithmetic_mean).mean().item()
            
            return analysis
    
    def _perceptual_analysis(self, waveform: torch.Tensor) -> Dict:
        """æ„ŸçŸ¥å“è³ªåˆ†æ"""
        with torch.no_grad():
            analysis = {}
            
            # Melé »è­œåˆ†æ
            mel_transform = T.MelSpectrogram(
                sample_rate=self.config.sr,
                n_fft=self.config.n_fft,
                hop_length=self.config.hop_length,
                n_mels=self.config.n_mels
            ).to(self.device)
            
            mel_spec = mel_transform(waveform)
            
            # MFCCç‰¹å¾µ
            mfcc_transform = T.MFCC(
                sample_rate=self.config.sr,
                n_mfcc=13,
                melkwargs={
                    'n_fft': self.config.n_fft,
                    'hop_length': self.config.hop_length,
                    'n_mels': self.config.n_mels
                }
            ).to(self.device)
            
            mfcc = mfcc_transform(waveform)
            
            # MFCCçµ±è¨ˆ
            analysis['mfcc_mean'] = torch.mean(mfcc, dim=(-2, -1)).cpu().numpy().tolist()
            analysis['mfcc_std'] = torch.std(mfcc, dim=(-2, -1)).cpu().numpy().tolist()
            
            # èªéŸ³æ´»å‹•æª¢æ¸¬
            energy = torch.sum(mel_spec, dim=-2)
            energy_threshold = torch.quantile(energy, 0.3)
            voice_activity = (energy > energy_threshold).float()
            analysis['voice_activity_ratio'] = torch.mean(voice_activity).item()
            
            return analysis
    
    def _dynamic_range_analysis(self, waveform: torch.Tensor) -> Dict:
        """å‹•æ…‹ç¯„åœåˆ†æ"""
        with torch.no_grad():
            analysis = {}
            
            # çŸ­æ™‚èƒ½é‡
            frame_length = self.config.hop_length
            frames = waveform.unfold(-1, frame_length, frame_length)
            frame_energy = torch.mean(frames ** 2, dim=-1)
            
            # å‹•æ…‹ç¯„åœçµ±è¨ˆ
            energy_db = 10 * torch.log10(frame_energy + 1e-10)
            analysis['dynamic_range'] = (torch.max(energy_db) - torch.min(energy_db)).item()
            analysis['energy_variance'] = torch.var(energy_db).item()
            
            # LUFSä¼°ç®—
            # ç°¡åŒ–çš„LUFSè¨ˆç®—
            rms_level = 20 * torch.log10(torch.sqrt(torch.mean(waveform ** 2)) + 1e-10)
            analysis['estimated_lufs'] = rms_level.item() - 23  # ç²—ç•¥è½‰æ›
            
            return analysis


def main():
    """ä¸»å‡½æ•¸ - å…ˆé€²éŸ³é »è™•ç†ç³»çµ±"""
    try:
        print("ğŸš€ å…ˆé€²éŸ³é »è™•ç†ç³»çµ± v2.0 å•Ÿå‹•...")
        print("=" * 80)
        
        # å‰µå»ºæœ€å„ªåŒ–é…ç½®
        config = create_optimal_config()
        
        # é¡¯ç¤ºç³»çµ±ä¿¡æ¯
        print("\nğŸ’» ç³»çµ±ä¿¡æ¯:")
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU: {gpu_name}")
            print(f"   GPUè¨˜æ†¶é«”: {gpu_memory:.1f}GB")
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        else:
            print("   ä½¿ç”¨CPUè™•ç†")
        
        print(f"   CPUæ ¸å¿ƒæ•¸: {psutil.cpu_count()}")
        print(f"   ç³»çµ±è¨˜æ†¶é«”: {psutil.virtual_memory().total / 1024**3:.1f}GB")
        
        # é¡¯ç¤ºé…ç½®
        print("\nâš™ï¸  è™•ç†é…ç½®:")
        print(f"   æ¡æ¨£ç‡: {config.sr} Hz")
        print(f"   FFTçª—å£: {config.n_fft}")
        print(f"   Melæ¿¾æ³¢å™¨: {config.n_mels}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config.max_batch_size}")
        print(f"   å·¥ä½œé€²ç¨‹: {config.num_workers}")
        print(f"   ç¥ç¶“å¢å¼·: {'âœ…' if config.use_neural_enhancement else 'âŒ'}")
        print(f"   æ··åˆç²¾åº¦: {'âœ…' if config.use_mixed_precision else 'âŒ'}")
        print(f"   å¿ƒç†è²å­¸å»ºæ¨¡: {'âœ…' if config.psychoacoustic_modeling else 'âŒ'}")
        
        # è¨­ç½®ç›®éŒ„è·¯å¾‘
        base_dir = Path('.')
        dataset_paths = {
            'train1': {
                'input': base_dir / 'dataset' / 'TRAINGING_DATASET_1' / 'Training_Dataset_01' / 'audio',
                'output': base_dir / 'processed_audio_v2' / 'train1'
            },
            'train2': {
                'input': base_dir / 'dataset' / 'Training_Dataset_02' / 'audio',
                'output': base_dir / 'processed_audio_v2' / 'train2'
            },
            'validation': {
                'input': base_dir / 'dataset' / 'TRAINGING_DATASET_1' / 'Validation_Dataset' / 'audio',
                'output': base_dir / 'processed_audio_v2' / 'validation'
            }
        }
        
        # åˆå§‹åŒ–è™•ç†å™¨
        processor = AdvancedAudioProcessor(config)
        
        print("\nğŸµ å…ˆé€²éŸ³é »è™•ç†ç‰¹æ€§:")
        print("   â€¢ Transformerç¥ç¶“ç¶²çµ¡å¢å¼·")
        print("   â€¢ å¿ƒç†è²å­¸å»ºæ¨¡")
        print("   â€¢ è‡ªé©æ‡‰å‹•æ…‹ç¯„åœæ§åˆ¶")
        print("   â€¢ å¤šé »æ®µæ™ºèƒ½å£“ç¸®")
        print("   â€¢ GPUæ··åˆç²¾åº¦åŠ é€Ÿ")
        print("   â€¢ ä¸¦è¡Œæ‰¹æ¬¡è™•ç†")
        print("   â€¢ å¯¦æ™‚å“è³ªè©•ä¼°")
        print("   â€¢ æ™ºèƒ½åƒæ•¸è‡ªé©æ‡‰")
        print("=" * 80)
        
        total_success = 0
        processing_summary = {}
        overall_start_time = time.time()
        
        # è™•ç†å„å€‹æ•¸æ“šé›†
        for dataset_name, paths in dataset_paths.items():
            input_dir = paths['input']
            output_dir = paths['output']
            
            if not input_dir.exists():
                logger.warning(f"è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
                continue
            
            print(f"\nğŸ¯ é–‹å§‹è™•ç† {dataset_name.upper()} æ•¸æ“šé›†...")
            print(f"ğŸ“ è¼¸å…¥: {input_dir}")
            print(f"ğŸ“ è¼¸å‡º: {output_dir}")
            
            # ä¸¦è¡Œè™•ç†æ•¸æ“šé›†
            success_count = processor.process_dataset_parallel(input_dir, output_dir)
            total_success += success_count
            
            processing_summary[dataset_name] = {
                'success_count': success_count,
                'input_dir': str(input_dir),
                'output_dir': str(output_dir)
            }
            
            print(f"âœ… {dataset_name.upper()} è™•ç†å®Œæˆ: {success_count} å€‹æ–‡ä»¶")
            
            # æ¸…ç†GPUè¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        total_processing_time = time.time() - overall_start_time
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        print("\n" + "=" * 80)
        print("ğŸ† å…ˆé€²éŸ³é »è™•ç†ç³»çµ± - æœ€çµ‚å ±å‘Š")
        print("=" * 80)
        
        print(f"ğŸ“Š ç¸½è™•ç†æ–‡ä»¶: {total_success}")
        print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {total_processing_time/60:.1f} åˆ†é˜")
        
        for dataset_name, summary in processing_summary.items():
            print(f"ğŸ“ {dataset_name.upper()}: {summary['success_count']} æ–‡ä»¶")
        
        if processor.performance_stats['throughput_rates']:
            avg_throughput = np.mean(processor.performance_stats['throughput_rates'])
            peak_throughput = np.max(processor.performance_stats['throughput_rates'])
            print(f"ğŸš€ å¹³å‡è™•ç†é€Ÿåº¦: {avg_throughput:.1f}x å¯¦æ™‚")
            print(f"ğŸ”¥ å³°å€¼è™•ç†é€Ÿåº¦: {peak_throughput:.1f}x å¯¦æ™‚")
        
        if processor.performance_stats['quality_scores']:
            avg_quality = np.mean(processor.performance_stats['quality_scores'])
            min_quality = np.min(processor.performance_stats['quality_scores'])
            print(f"ğŸ¯ å¹³å‡å“è³ªåˆ†æ•¸: {avg_quality:.3f}")
            print(f"ğŸ“‰ æœ€ä½å“è³ªåˆ†æ•¸: {min_quality:.3f}")
        
        # ä¿å­˜å…¨å±€çµ±è¨ˆ
        global_stats = {
            'total_files_processed': total_success,
            'total_processing_time': total_processing_time,
            'datasets_processed': processing_summary,
            'performance_stats': {
                'average_throughput': np.mean(processor.performance_stats['throughput_rates']) if processor.performance_stats['throughput_rates'] else 0,
                'peak_throughput': np.max(processor.performance_stats['throughput_rates']) if processor.performance_stats['throughput_rates'] else 0,
                'average_quality': np.mean(processor.performance_stats['quality_scores']) if processor.performance_stats['quality_scores'] else 0
            },
            'system_config': {
                'device': str(processor.device),
                'mixed_precision': config.use_mixed_precision,
                'neural_enhancement': config.use_neural_enhancement,
                'sample_rate': config.sr,
                'processing_mode': 'advanced_v2'
            },
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        stats_file = base_dir / 'processed_audio_v2' / 'global_processing_stats.json'
        stats_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(global_stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ å…¨å±€çµ±è¨ˆå·²ä¿å­˜è‡³: {stats_file}")
        
        # æŠ€è¡“ç¸½çµ
        print("\nğŸ”¬ æŠ€è¡“äº®é»:")
        print("   âœ¨ åŸºæ–¼Transformerçš„æ™ºèƒ½é™å™ª")
        print("   ğŸ§  å¿ƒç†è²å­¸æ„ŸçŸ¥å»ºæ¨¡")
        print("   âš¡ GPUæ··åˆç²¾åº¦è¨ˆç®—")
        print("   ğŸ›ï¸  è‡ªé©æ‡‰åƒæ•¸èª¿æ•´")
        print("   ğŸ“Š å¯¦æ™‚å“è³ªç›£æ§")
        print("   ğŸ”„ å…§å­˜å„ªåŒ–æ‰¹è™•ç†")
        print("   ğŸµ ä¿çœŸåº¦éŸ³é »é‡å»º")
        
        # æ€§èƒ½è©•ä¼°
        print("\nğŸ“ˆ æ€§èƒ½è©•ä¼°:")
        if processor.performance_stats['throughput_rates']:
            avg_speed = np.mean(processor.performance_stats['throughput_rates'])
            if avg_speed > 20:
                print("ğŸš€ æ€§èƒ½ç­‰ç´š: æ¥µä½³ (>20xå¯¦æ™‚)")
            elif avg_speed > 10:
                print("âœ… æ€§èƒ½ç­‰ç´š: å„ªç§€ (10-20xå¯¦æ™‚)")
            elif avg_speed > 5:
                print("ğŸ‘ æ€§èƒ½ç­‰ç´š: è‰¯å¥½ (5-10xå¯¦æ™‚)")
            else:
                print("âš ï¸  æ€§èƒ½ç­‰ç´š: ä¸€èˆ¬ (<5xå¯¦æ™‚)")
        
        if processor.performance_stats['quality_scores']:
            avg_quality = np.mean(processor.performance_stats['quality_scores'])
            if avg_quality > 0.9:
                print("ğŸ¯ éŸ³è³ªç­‰ç´š: å“è¶Š (>0.9)")
            elif avg_quality > 0.8:
                print("âœ… éŸ³è³ªç­‰ç´š: å„ªç§€ (0.8-0.9)")
            elif avg_quality > 0.7:
                print("ğŸ‘ éŸ³è³ªç­‰ç´š: è‰¯å¥½ (0.7-0.8)")
            else:
                print("âš ï¸  éŸ³è³ªç­‰ç´š: éœ€è¦æ”¹é€² (<0.7)")
        
        print("\nğŸ‰ æ‰€æœ‰éŸ³é »è™•ç†å®Œæˆï¼")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ AudioQualityAnalyzer å¯ä»¥ç²å¾—æ›´è©³ç´°çš„éŸ³é »å“è³ªåˆ†æ")
        
    except Exception as e:
        print(f"âŒ ç³»çµ±é‹è¡Œå¤±æ•—: {e}")
        logger.error(f"ç³»çµ±é‹è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹•å…ˆé€²éŸ³é »è™•ç†ç³»çµ± v2.0...")
    main()