{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOHV3tgnd2Mp",
    "outputId": "9432ab86-3524-40b6-8e48-dc7681f0a796"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5_ZSvBNdHQ9",
    "outputId": "4047a245-0ce0-4aa1-f4aa-fc20632ec638"
   },
   "outputs": [],
   "source": [
    "!pip install torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_token:  \n",
      "task1_train_data_path:  ./data/train.json\n",
      "task2_train_data_path:  ./data/train.json\n",
      "task1_val_data_path:  ./data/train.json\n",
      "task2_val_data_path:  ./data/train.json\n",
      "model_save_path:  ./NER_model\n",
      "model_logging_dir:  ./ner_logs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config_argument = json.load(f)\n",
    "\n",
    "access_token = config_argument[\"huggingface_access_token\"]\n",
    "\n",
    "task1_train_data_path_txt = config_argument[\"model_train_task1_data_path_txt\"]\n",
    "task2_train_data_path_txt = config_argument[\"model_train_task2_data_path_txt\"]\n",
    "\n",
    "task1_val_data_path_txt = config_argument[\"model_val_task1_data_path_txt\"]\n",
    "task2_val_data_path_txt = config_argument[\"model_val_task2_data_path_txt\"]\n",
    "\n",
    "answer_val_data_path_txt = config_argument[\"answer_val_data_path_txt\"]\n",
    "\n",
    "model_save_path = config_argument[\"model_save_path\"]\n",
    "model_logging_dir = config_argument[\"model_logging_dir\"]\n",
    "\n",
    "\n",
    "print( \"access_token: \", access_token )\n",
    "print( \"task1_train_data_path: \", task1_train_data_path_txt )\n",
    "print( \"task2_train_data_path: \", task2_train_data_path_txt )\n",
    "print( \"task1_val_data_path: \", task1_val_data_path_txt )\n",
    "print( \"task2_val_data_path: \", task2_val_data_path_txt )\n",
    "print( \"answer_val_data_path: \", answer_val_data_path_txt )\n",
    "print( \"model_save_path: \", model_save_path )\n",
    "print( \"model_logging_dir: \", model_logging_dir )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeWzvMM9FrOA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torchcrf import CRF\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from transformers import XLMRobertaConfig, XLMRobertaModel, XLMRobertaPreTrainedModel\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import XLMRobertaForTokenClassification\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "\n",
    "class XLMRobertaWithCRF(XLMRobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # 用 from_pretrained 來載入預訓練權重\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", config=config, token=access_token)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            mask = attention_mask.bool()\n",
    "            mask[:, 0] = True  # 確保第一token是有效的mask\n",
    "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=emissions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jy5Zc_v4hJni"
   },
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'O',\n",
    "    1: 'B-PATIENT', 2: 'I-PATIENT',\n",
    "    3: 'B-DOCTOR', 4: 'I-DOCTOR',\n",
    "    5: 'B-USERNAME', 6: 'I-USERNAME',\n",
    "    7: 'B-FAMILYNAME', 8: 'I-FAMILYNAME',\n",
    "    9: 'B-PERSONALNAME', 10: 'I-PERSONALNAME',\n",
    "    11: 'B-PROFESSION', 12: 'I-PROFESSION',\n",
    "    13: 'B-ROOM', 14: 'I-ROOM',\n",
    "    15: 'B-DEPARTMENT', 16: 'I-DEPARTMENT',\n",
    "    17: 'B-HOSPITAL', 18: 'I-HOSPITAL',\n",
    "    19: 'B-ORGANIZATION', 20: 'I-ORGANIZATION',\n",
    "    21: 'B-STREET', 22: 'I-STREET',\n",
    "    23: 'B-CITY', 24: 'I-CITY',\n",
    "    25: 'B-DISTRICT', 26: 'I-DISTRICT',\n",
    "    27: 'B-COUNTY', 28: 'I-COUNTY',\n",
    "    29: 'B-STATE', 30: 'I-STATE',\n",
    "    31: 'B-COUNTRY', 32: 'I-COUNTRY',\n",
    "    33: 'B-ZIP', 34: 'I-ZIP',\n",
    "    35: 'B-LOCATION-OTHER', 36: 'I-LOCATION-OTHER',\n",
    "    37: 'B-AGE', 38: 'I-AGE',\n",
    "    39: 'B-DATE', 40: 'I-DATE',\n",
    "    41: 'B-TIME', 42: 'I-TIME',\n",
    "    43: 'B-DURATION', 44: 'I-DURATION',\n",
    "    45: 'B-SET', 46: 'I-SET',\n",
    "    47: 'B-PHONE', 48: 'I-PHONE',\n",
    "    49: 'B-FAX', 50: 'I-FAX',\n",
    "    51: 'B-EMAIL', 52: 'I-EMAIL',\n",
    "    53: 'B-URL', 54: 'I-URL',\n",
    "    55: 'B-IPADDRESS', 56: 'I-IPADDRESS',\n",
    "    57: 'B-SOCIAL_SECURITY_NUMBER', 58: 'I-SOCIAL_SECURITY_NUMBER',\n",
    "    59: 'B-MEDICAL_RECORD_NUMBER', 60: 'I-MEDICAL_RECORD_NUMBER',\n",
    "    61: 'B-HEALTH_PLAN_NUMBER', 62: 'I-HEALTH_PLAN_NUMBER',\n",
    "    63: 'B-ACCOUNT_NUMBER', 64: 'I-ACCOUNT_NUMBER',\n",
    "    65: 'B-LICENSE_NUMBER', 66: 'I-LICENSE_NUMBER',\n",
    "    67: 'B-VEHICLE_ID', 68: 'I-VEHICLE_ID',\n",
    "    69: 'B-DEVICE_ID', 70: 'I-DEVICE_ID',\n",
    "    71: 'B-BIOMETRIC_ID', 72: 'I-BIOMETRIC_ID',\n",
    "    73: 'B-ID_NUMBER', 74: 'I-ID_NUMBER',\n",
    "    75: 'B-OTHER', 76: 'I-OTHER',\n",
    "    77: 'IGNORE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUhHM7jMQmSg"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = access_token )\n",
    "\n",
    "from transformers import XLMRobertaConfig\n",
    "\n",
    "config = XLMRobertaConfig.from_pretrained(model_name, num_labels=len(label_map))\n",
    "model = XLMRobertaWithCRF(config)\n",
    "\n",
    "\n",
    "model.config.id2label = label_map\n",
    "model.config.label2id = {v:k for k,v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaFPkMh-T6Jt",
    "outputId": "f02e45f2-9579-49ea-cc85-f45b59d51b15"
   },
   "outputs": [],
   "source": [
    "print(\"新的 id2label:\", model.config.id2label)\n",
    "print(\"新的 label2id:\", model.config.label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pJ4iAcwmSQO"
   },
   "source": [
    "# **資料準備**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SdsW_ccnnwr"
   },
   "outputs": [],
   "source": [
    "def Caculate_Wav_File_Times( inputs ) :\n",
    "\n",
    "        read = inputs\n",
    "\n",
    "        dict_times = {}\n",
    "        for line in read:\n",
    "            line = line.strip()\n",
    "            line_split = line.split('\\t')\n",
    "\n",
    "            if line_split[0] not in dict_times :\n",
    "                dict_times[line_split[0]] = 1\n",
    "            else:\n",
    "                dict_times[line_split[0]] = dict_times[line_split[0]]  + 1\n",
    "\n",
    "        return dict_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFDTcWSqmRxS"
   },
   "outputs": [],
   "source": [
    "# with open( \"/content/task2_answer.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = f.readlines()\n",
    "\n",
    "def Prepare_Task2_NER(data) :\n",
    "  data_times_dict = Caculate_Wav_File_Times( data )\n",
    "\n",
    "\n",
    "  data_list = {}\n",
    "  temp_dict = {}\n",
    "  temp_list = []\n",
    "\n",
    "  while data :\n",
    "\n",
    "    times = data_times_dict[data[0].split('\\t')[0]]\n",
    "\n",
    "    for i in range( times  ) :\n",
    "\n",
    "      line = data[i]\n",
    "\n",
    "\n",
    "      line = line.strip()\n",
    "      line_split = line.split(\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "      temp_dict[ line_split[4] ] = line_split[1]\n",
    "      temp_list.append( temp_dict )\n",
    "      temp_dict = {}\n",
    "\n",
    "    data_list[ data[0].split('\\t')[0] ] = temp_list\n",
    "    temp_list = []\n",
    "\n",
    "\n",
    "    data = data[times:]\n",
    "\n",
    "\n",
    "  print(data_list)\n",
    "\n",
    "  return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaQEOcNxpIgS",
    "outputId": "9e2dd937-2b9c-44a4-862d-929c51ffd220"
   },
   "outputs": [],
   "source": [
    "#en\n",
    "\n",
    "with open( task2_train_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
    "  data = f.readlines()\n",
    "\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_1/task2_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = f.readlines()\n",
    "\n",
    "# print(len(data))\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_2/task2_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = data + f.readlines()\n",
    "\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_3/task2_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = data + f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "data_list = Prepare_Task2_NER( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHUSt8h84bXs",
    "outputId": "2a5e7dae-2873-4c4d-d9cf-7e8320cedb1e"
   },
   "outputs": [],
   "source": [
    "#en\n",
    "\n",
    "with open( task2_val_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
    "  val_data = f.readlines()\n",
    "\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/test_set/task2_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   val_data = f.readlines()\n",
    "\n",
    "print(len(val_data))\n",
    "\n",
    "\n",
    "val_data_list = Prepare_Task2_NER( val_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "othNXXXQs0Gp"
   },
   "outputs": [],
   "source": [
    "def Prepare_Task1_NER( data, data_list):\n",
    "  train_data = []\n",
    "\n",
    "  for i in data:\n",
    "      # print( i )\n",
    "      line = i.strip()\n",
    "      line_split = line.split(\"\\t\")\n",
    "\n",
    "      name = line_split[0]\n",
    "      text = line_split[1]\n",
    "\n",
    "      tokens = tokenizer(text.strip(), return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
    "      offsets = tokens[\"offset_mapping\"][0].tolist()\n",
    "      input_ids = tokens[\"input_ids\"][0].tolist()\n",
    "      token_texts = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "      # 初始化 label\n",
    "      label = [config.label2id[\"O\"]] * len(input_ids)\n",
    "      label[0] = 77\n",
    "      label[-1] = 77\n",
    "\n",
    "\n",
    "      input_ids = tokens[\"input_ids\"]\n",
    "      attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "      # print(f\"name: {name}\")\n",
    "      # print(f\"text: {text}\")\n",
    "      # print(f\"offsets: {offsets}\")\n",
    "      # print(tokens.tokens())\n",
    "\n",
    "\n",
    "      # 檢查是否有標註資料\n",
    "      if name not in data_list:\n",
    "          train_data.append({\n",
    "              \"input_ids\": input_ids[0].tolist(),\n",
    "              \"labels\": label,\n",
    "              \"attention_mask\": attention_mask[0].tolist()\n",
    "          })\n",
    "          continue\n",
    "\n",
    "      # 將標註合併為 (start, end, tag) 的格式\n",
    "      entities = []\n",
    "      used_indices = set()  # 防止重複使用相同文字\n",
    "      for ent in data_list[name]:\n",
    "          for word, tag in ent.items():\n",
    "              # 用 sliding window 尋找沒使用過的 word 位置\n",
    "              start = -1\n",
    "              for idx in range(len(text)):\n",
    "                  if idx in used_indices:\n",
    "                      continue\n",
    "                  if text[idx:idx+len(word)] == word:\n",
    "                      start = idx\n",
    "                      # 標記這些字元位置已經用過\n",
    "                      used_indices.update(range(start, start+len(word)))\n",
    "                      break\n",
    "              if start != -1:\n",
    "                  end = start + len(word)\n",
    "                  entities.append((start, end, tag))\n",
    "              else:\n",
    "                  print(f\"[未找到實體] name={name}, word='{word}', tag='{tag}'\")\n",
    "                  print(f\"→ 原始句子：{text}\")\n",
    "                  print(text[idx:idx+len(word)])\n",
    "\n",
    "      # print(f\"name: {name}\")\n",
    "      # print(f\"text: {text}\")\n",
    "      # print(f\"entities: {entities}\")\n",
    "\n",
    "      # 比對 offset 和 entity span，標註 label\n",
    "      for idx, (start, end) in enumerate(offsets):\n",
    "          if start == end:\n",
    "              continue\n",
    "          for ent_start, ent_end, tag in entities:\n",
    "              if start == ent_start:\n",
    "                  label[idx] = config.label2id[f\"B-{tag}\"]\n",
    "                  break\n",
    "              elif ent_start < start < ent_end:\n",
    "                  label[idx] = config.label2id[f\"I-{tag}\"]\n",
    "                  break\n",
    "\n",
    "      train_data.append({\n",
    "          \"input_ids\": input_ids[0].tolist(),\n",
    "          \"labels\": label,\n",
    "          \"attention_mask\": attention_mask[0].tolist()\n",
    "      })\n",
    "\n",
    "  return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7GcVzcgPvMY",
    "outputId": "6d98cd46-a775-499e-c166-2694505fe394"
   },
   "outputs": [],
   "source": [
    "#en\n",
    "with open( task1_train_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
    "  data = f.readlines()\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_1/task1_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = f.readlines()\n",
    "\n",
    "# print( len(data) )\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_2/task1_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = data + f.readlines()\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_3/task1_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   data = data + f.readlines()\n",
    "\n",
    "\n",
    "print( len(data) )\n",
    "\n",
    "train_data = Prepare_Task1_NER( data, data_list )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOMDcgpR5_Ap",
    "outputId": "24f33169-37ca-498c-f33e-0741163b928c"
   },
   "outputs": [],
   "source": [
    "#en\n",
    "with open( task1_val_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
    "  val_data = f.readlines()\n",
    "\n",
    "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/test_set/task1_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
    "#   val_data = f.readlines()\n",
    "\n",
    "print( len(val_data) )\n",
    "\n",
    "\n",
    "test_data = Prepare_Task1_NER( val_data, val_data_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7R4zkKLPLpy",
    "outputId": "724975dc-5770-4003-bc7d-2e78bf0e2efe"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXXHK3Ts9uXB"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data_train = Dataset.from_list(train_data)\n",
    "data_test = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bXWWzQuPRFC",
    "outputId": "9947f6dd-1b32-4ba7-8d6f-12287a99a5bc"
   },
   "outputs": [],
   "source": [
    "data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o1-mtRzSmTw"
   },
   "source": [
    "# **TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNvzAuVzcagW",
    "outputId": "6ce97cfb-1511-478f-df51-498417fb485e"
   },
   "outputs": [],
   "source": [
    "print(train_data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBfKT7zv5euX"
   },
   "outputs": [],
   "source": [
    "def Process_Predict_Ner(pre):\n",
    "    answer_list = []\n",
    "    current_entity = None\n",
    "    current_word = \"\"\n",
    "    start_pos = None\n",
    "    end_pos = None\n",
    "\n",
    "    for dic in pre:\n",
    "        entity_type = dic['entity']\n",
    "        raw_word = dic['word']\n",
    "        word = raw_word.replace(\"▁\", \"\")\n",
    "        token_start = dic.get('start')\n",
    "        token_end = dic.get('end')\n",
    "        has_space = raw_word.startswith(\"▁\")\n",
    "\n",
    "        if entity_type.startswith(\"B-\"):\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = entity_type.replace(\"B-\", \"\")\n",
    "            current_word = word\n",
    "            start_pos = token_start\n",
    "            end_pos = token_end\n",
    "\n",
    "        elif entity_type.startswith(\"I-\"):\n",
    "            ent = entity_type.replace(\"I-\", \"\")\n",
    "            if current_entity == ent:\n",
    "                if has_space:\n",
    "                    current_word += \" \" + word\n",
    "                else:\n",
    "                    current_word += word\n",
    "                end_pos = token_end\n",
    "            else:\n",
    "                if current_entity and current_word:\n",
    "                    answer_list.append({\n",
    "                        \"entity\": current_entity,\n",
    "                        \"word\": current_word,\n",
    "                        \"start\": start_pos,\n",
    "                        \"end\": end_pos\n",
    "                    })\n",
    "                current_entity = ent\n",
    "                current_word = word\n",
    "                start_pos = token_start\n",
    "                end_pos = token_end\n",
    "\n",
    "        else:  # O\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = None\n",
    "            current_word = \"\"\n",
    "            start_pos = None\n",
    "            end_pos = None\n",
    "\n",
    "    # 收尾\n",
    "    if current_entity and current_word:\n",
    "        answer_list.append({\n",
    "            \"entity\": current_entity,\n",
    "            \"word\": current_word,\n",
    "            \"start\": start_pos,\n",
    "            \"end\": end_pos\n",
    "        })\n",
    "\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-WjOySg7i67"
   },
   "outputs": [],
   "source": [
    "# 小心 -100 和 0   \"labels\": pad_sequence(labels, batch_first=True, padding_value=0)  # Changed padding_value to -100\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def custom_collator(features):\n",
    "    input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "    attention_mask = [torch.tensor(f[\"attention_mask\"]) for f in features]\n",
    "    labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
    "\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id),\n",
    "        \"attention_mask\": pad_sequence(attention_mask, batch_first=True, padding_value=0),\n",
    "        \"labels\": pad_sequence(labels, batch_first=True, padding_value=77)  # Changed padding_value to -100\n",
    "    }\n",
    "\n",
    "    # print( batch )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24X8mvIh6H9b"
   },
   "outputs": [],
   "source": [
    "def get_level2_entities_normal(model, tokenizer, sentence, label_map):\n",
    "    device = next(model.parameters()).device  # 取得 model 裝置\n",
    "\n",
    "    # 1. Tokenize with offsets\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "    input_ids = encoding[\"input_ids\"].to(device)          # 放到 GPU\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)  # 放到 GPU\n",
    "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu())  # token ids 放 CPU 才能用 tokenizer\n",
    "\n",
    "    # 2. Model forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, num_labels]\n",
    "\n",
    "    preds = torch.argmax(logits, dim=2)[0].cpu().numpy()  # 預測結果放回 CPU\n",
    "\n",
    "    results = []\n",
    "    for idx, (pred_id, offset) in enumerate(zip(preds, offsets)):\n",
    "        token_id = input_ids[0, idx].item()\n",
    "\n",
    "        # 跳過特殊 token 或無效 offset\n",
    "        if token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "            continue\n",
    "\n",
    "        start, end = offset\n",
    "        entity = label_map.get(pred_id, \"O\")\n",
    "\n",
    "        if entity != \"O\":\n",
    "            probs = torch.softmax(logits[0, idx], dim=0)\n",
    "            score = probs[pred_id].item()\n",
    "\n",
    "            results.append({\n",
    "                \"entity\": entity,\n",
    "                \"score\": np.float32(score),\n",
    "                \"index\": idx,\n",
    "                \"word\": tokens[idx],  # 更準確\n",
    "                \"start\": start,\n",
    "                \"end\": end\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-yNK0Rn5yYo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_overlap(pred_start, pred_end, gt_start, gt_end):\n",
    "    \"\"\"計算兩個時間區間的重疊長度\"\"\"\n",
    "    overlap_start = max(pred_start, gt_start)\n",
    "    overlap_end = min(pred_end, gt_end)\n",
    "    overlap = max(0, overlap_end - overlap_start)\n",
    "    return overlap\n",
    "\n",
    "def evaluate_task2( ground_truth_file, model, tokenizer ) :\n",
    "\n",
    "\n",
    "\n",
    "    answer = \"\"\n",
    "\n",
    "    for text in val_data :\n",
    "\n",
    "      answer_list = []\n",
    "\n",
    "      text_split = text.strip().split(\"\\t\")\n",
    "      name = text_split[0]\n",
    "      text = text_split[1]\n",
    "\n",
    "      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "      if len(pre) != 0:\n",
    "        answer_list = Process_Predict_Ner(pre)\n",
    "\n",
    "      for i in answer_list:\n",
    "        answer += f\"{name}\\t{i['entity']}\\t{i['start']}\\t{i['end']}\\t{i['word']}\\n\"\n",
    "\n",
    "    prediction_file = \"model_eval.txt\"\n",
    "    with open( prediction_file, \"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(answer)\n",
    "\n",
    "\n",
    "    # 讀取預測和真實標籤數據\n",
    "    import csv\n",
    "    pred_df = pd.read_csv(\n",
    "          prediction_file,\n",
    "          sep='\\t',\n",
    "          header=None,\n",
    "          names=['id', 'type', 'start', 'end', 'content'],\n",
    "          quoting=csv.QUOTE_NONE,        # 不解析引號\n",
    "          encoding='utf-8',              # 或試 utf-8-sig\n",
    "          on_bad_lines='skip',           # 跳過爛行\n",
    "          engine='python'                # 更寬容的 parser\n",
    "      )\n",
    "    gt_df = pd.read_csv(ground_truth_file, sep='\\t', header=None,\n",
    "                       names=['id', 'type', 'start', 'end', 'content'])\n",
    "\n",
    "    # 獲取所有獨特的SHI類型\n",
    "    all_types = sorted(set(gt_df['type'].unique()) | set(pred_df['type'].unique()))\n",
    "\n",
    "    # 初始化每種類型的指標\n",
    "    metrics = {shi_type: {'tp': 0, 'fp': 0, 'fn': 0} for shi_type in all_types}\n",
    "\n",
    "    # 按音頻ID分組處理\n",
    "    unique_ids = sorted(set(gt_df['id'].unique()) | set(pred_df['id'].unique()))\n",
    "\n",
    "    for audio_id in unique_ids:\n",
    "        gt_records = gt_df[gt_df['id'] == audio_id].copy()\n",
    "        pred_records = pred_df[pred_df['id'] == audio_id].copy()\n",
    "\n",
    "        # 初始化匹配矩陣來追蹤已處理的預測和真實標籤\n",
    "        gt_matched = [False] * len(gt_records)\n",
    "        pred_matched = [False] * len(pred_records)\n",
    "\n",
    "        # 計算True Positives和部分False Positives/False Negatives\n",
    "        for i, pred_row in enumerate(pred_records.itertuples()):\n",
    "            pred_type = pred_row.type\n",
    "            pred_start = pred_row.start\n",
    "            pred_end = pred_row.end\n",
    "            pred_duration = pred_end - pred_start\n",
    "\n",
    "            best_overlap = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            # 找到與當前預測重疊最大的真實標籤\n",
    "            for j, gt_row in enumerate(gt_records.itertuples()):\n",
    "                if gt_row.type != pred_type:\n",
    "                    continue\n",
    "\n",
    "                overlap = calculate_overlap(pred_start, pred_end, gt_row.start, gt_row.end)\n",
    "                if overlap > best_overlap:\n",
    "                    best_overlap = overlap\n",
    "                    best_gt_idx = j\n",
    "\n",
    "            if best_gt_idx >= 0:  # 找到部分匹配\n",
    "                gt_row = gt_records.iloc[best_gt_idx]\n",
    "                gt_duration = gt_row.end - gt_row.start\n",
    "\n",
    "                # 計算 True Positive\n",
    "                metrics[pred_type]['tp'] += best_overlap\n",
    "\n",
    "                # 計算 False Positive (對於部分匹配，類型相同)\n",
    "                metrics[pred_type]['fp'] += pred_duration - best_overlap\n",
    "\n",
    "                # 計算 False Negative (對於部分匹配，類型相同)\n",
    "                metrics[pred_type]['fn'] += gt_duration - best_overlap\n",
    "\n",
    "                # 標記已處理\n",
    "                gt_matched[best_gt_idx] = True\n",
    "                pred_matched[i] = True\n",
    "            else:\n",
    "                # 完全不匹配或者類型不同：整個預測為False Positive\n",
    "                metrics[pred_type]['fp'] += pred_duration\n",
    "\n",
    "        # 處理未匹配的真實標籤 (False Negatives)\n",
    "        for j, matched in enumerate(gt_matched):\n",
    "            if not matched:\n",
    "                gt_row = gt_records.iloc[j]\n",
    "                gt_type = gt_row.type\n",
    "                gt_duration = gt_row.end - gt_row.start\n",
    "                metrics[gt_type]['fn'] += gt_duration\n",
    "\n",
    "        # 處理與類型不同的預測 (False Positives)\n",
    "        for i, (matched, pred_row) in enumerate(zip(pred_matched, pred_records.itertuples())):\n",
    "            if matched:\n",
    "                continue\n",
    "\n",
    "            # 檢查是否有與其他類型匹配\n",
    "            pred_type = pred_row.type\n",
    "            pred_start = pred_row.start\n",
    "            pred_end = pred_row.end\n",
    "            pred_duration = pred_end - pred_start\n",
    "\n",
    "            for gt_row in gt_records.itertuples():\n",
    "                if gt_row.type == pred_type:\n",
    "                    continue  # 已在之前的步驟中處理過\n",
    "\n",
    "                overlap = calculate_overlap(pred_start, pred_end, gt_row.start, gt_row.end)\n",
    "                if overlap > 0:\n",
    "                    # 類型不匹配但時間重疊：整個預測為False Positive\n",
    "                    metrics[pred_type]['fp'] += pred_duration\n",
    "                    break\n",
    "\n",
    "    # 計算每種類型的Precision, Recall和F1\n",
    "    f1_scores = []\n",
    "    for shi_type in all_types:\n",
    "        m = metrics[shi_type]\n",
    "        precision = m['tp'] / (m['tp'] + m['fp']) if (m['tp'] + m['fp']) > 0 else 0\n",
    "        recall = m['tp'] / (m['tp'] + m['fn']) if (m['tp'] + m['fn']) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # print(f\"類型 {shi_type}:\")\n",
    "        # print(f\"  Precision: {precision:.4f}\")\n",
    "        # print(f\"  Recall: {recall:.4f}\")\n",
    "        # print(f\"  F1: {f1:.4f}\")\n",
    "        # print(f\"  TP: {m['tp']:.2f}, FP: {m['fp']:.2f}, FN: {m['fn']:.2f}\")\n",
    "        # print()\n",
    "\n",
    "    # 計算宏平均F1\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    # print(f\"Macro-Average F1: {macro_f1:.4f}\")\n",
    "\n",
    "    return macro_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxMxJIzEVZuV"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_save_path,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_first_step=True,\n",
    "    logging_dir= model_logging_dir ,\n",
    "    learning_rate=3e-5,                     # 微幅調高（視情況）3e-5\n",
    "    num_train_epochs=40,                   # 避免一次就設 50\n",
    "    weight_decay=0.03,                     # 適當正則化 0.03\n",
    "    # per_device_train_batch_size=8,        # 批量大一點也有助穩定\n",
    "    # per_device_eval_batch_size=8,\n",
    "    # load_best_model_at_end=True,           # ⚠️ 搭配 EarlyStopping 時很重要\n",
    "    # metric_for_best_model=\"eval_loss\",     # 根據 loss 選最佳模型\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKsiUc936jN6"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class CharBasedEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, task2_path, tokenizer):\n",
    "        self.task2_path = task2_path\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "\n",
    "        macro_f1 = evaluate_task2(self.task2_path, model, self.tokenizer)\n",
    "\n",
    "        print(f\"[Char-based Evaluation after epoch {state.epoch}]\")\n",
    "        print(\"Macro-F1:\", macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVZoqY6_8r4U"
   },
   "outputs": [],
   "source": [
    "class FGM:\n",
    "    def __init__(self, model, epsilon=1.0):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, emb_name='embeddings.word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = self.epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='embeddings.word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name and name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBilUoup8xh9"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class TrainerWithFGM(Trainer):\n",
    "    def __init__(self, *args, fgm=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fgm = fgm\n",
    "\n",
    "    def training_step(self, model, inputs, num_items):  # ← 加上 num_items\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # 原始 loss\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        loss.backward()\n",
    "\n",
    "        # 對抗訓練\n",
    "        if self.fgm is not None:\n",
    "            self.fgm.attack()\n",
    "            adv_loss = self.compute_loss(model, inputs)\n",
    "            adv_loss.backward()\n",
    "            self.fgm.restore()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lfv9J4AW2D6W",
    "outputId": "aba5bda1-4bb1-4fe7-e3d5-d0b24c34386f"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "task2_path = answer_val_data_path_txt\n",
    "\n",
    "\n",
    "fgm = FGM(model)\n",
    "\n",
    "trainer = TrainerWithFGM(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_test,\n",
    "    data_collator=custom_collator,\n",
    "    callbacks=[CharBasedEvaluationCallback(task2_path, tokenizer)],\n",
    "    fgm=fgm,  # ✅ 加入這裡\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
