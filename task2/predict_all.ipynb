{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_token:  \n",
      "model_1_path:  \n",
      "model_2_path:  \n",
      "model_3_path:  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config_argument = json.load(f)\n",
    "\n",
    "access_token = config_argument[\"huggingface_access_token\"]\n",
    "\n",
    "model_1_path = config_argument[\"model_checkpoint_1213\"]\n",
    "model_2_path = config_argument[\"model_checkpoint_1000\"]\n",
    "model_3_path = config_argument[\"model_checkpoint_500\"]\n",
    "\n",
    "\n",
    "model_test_task1_data_path_txt = config_argument[\"model_test_task1_data_path_txt\"]\n",
    "model_test_task2_data_path_txt = config_argument[\"model_test_task2_data_path_txt\"]\n",
    "\n",
    "print( \"access_token: \", access_token )\n",
    "\n",
    "print( \"model_1_path: \", model_1_path )\n",
    "print( \"model_2_path: \", model_2_path )\n",
    "print( \"model_3_path: \", model_3_path )\n",
    "\n",
    "print( \"model_test_task1_data_path_txt\", model_test_task1_data_path_txt )\n",
    "print( \"model_test_task2_data_path_txt\", model_test_task2_data_path_txt ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bULNVqHMACvf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Level1 對 Level2 的對應表\n",
    "check_ner = {\n",
    "    \"NAME\": [\"PATIENT\", \"DOCTOR\", \"USERNAME\", \"FAMILYNAME\", \"PERSONALNAME\"],\n",
    "    \"OCCUPATION\": [\"PROFESSION\"],\n",
    "    \"LOCATION\": [\"ROOM\", \"DEPARTMENT\", \"HOSPITAL\", \"ORGANIZATION\", \"STREET\", \"CITY\", \"DISTRICT\", \"COUNTY\", \"STATE\", \"COUNTRY\", \"ZIP\", \"LOCATION-OTHER\"],\n",
    "    \"AGE\": [\"AGE\"],\n",
    "    \"DATE\": [\"DATE\"],\n",
    "    \"CONTACT_INFORMATION\": [\"PHONE\", \"FAX\", \"EMAIL\", \"URL\", \"IPADDRESS\"],\n",
    "    \"IDENTIFIERS\": [\"SOCIAL_SECURITY_NUMBER\", \"MEDICAL_RECORD_NUMBER\", \"HEALTH_PLAN_NUMBER\", \"ACCOUNT_NUMBER\", \"LICENSE_NUMBER\", \"VEHICLE_ID\", \"DEVICE_ID\", \"BIOMETRIC_ID\", \"ID_NUMBER\"],\n",
    "    \"OTHER\": [\"OTHER\"]\n",
    "}\n",
    "\n",
    "def strip_prefix(tag):\n",
    "    \"\"\"去掉 BIO prefix，如 B-PATIENT → PATIENT\"\"\"\n",
    "    if tag.startswith(\"B-\") or tag.startswith(\"I-\"):\n",
    "        return tag[2:]\n",
    "    return tag\n",
    "\n",
    "\n",
    "\n",
    "# level2 屬於 level1 才可以\n",
    "def get_level2_entities_1(model, tokenizer, sentence, label_map_lvl1, label_map_lvl2, choose_loss):\n",
    "    # 1. Tokenize with offsets\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # 2. Model forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask , choose_loss=choose_loss)\n",
    "        logits_lvl1, logits_lvl2 = outputs.logits  # tuple of two tensors\n",
    "\n",
    "    # 3. Predict Level 1 & Level 2\n",
    "    preds_lvl1 = torch.argmax(logits_lvl1, dim=2)[0].cpu().numpy()\n",
    "    preds_lvl2 = torch.argmax(logits_lvl2, dim=2)[0].cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for idx, (pred1_id, pred2_id) in enumerate(zip(preds_lvl1, preds_lvl2)):\n",
    "        token_id = input_ids[0, idx].item()\n",
    "        if token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "            continue\n",
    "\n",
    "        start, end = offsets[idx]\n",
    "        if start == end:\n",
    "            continue\n",
    "\n",
    "        entity_lvl1 = label_map_lvl1.get(pred1_id, \"O\")\n",
    "        entity_lvl2 = label_map_lvl2.get(pred2_id, \"O\")\n",
    "        entity_lvl1_temp = strip_prefix(entity_lvl1)\n",
    "        entity_lvl2_temp = strip_prefix(entity_lvl2)\n",
    "\n",
    "        # ✅ 僅當 Level 1 為合法大類，且 Level 2 屬於其子類時\n",
    "        if entity_lvl1_temp in check_ner:\n",
    "            allowed_subtypes = check_ner[entity_lvl1_temp]\n",
    "            if entity_lvl2_temp in allowed_subtypes:\n",
    "                probs = torch.softmax(logits_lvl2[0, idx], dim=0)\n",
    "                score = float(probs[pred2_id].cpu().numpy())\n",
    "\n",
    "                results.append({\n",
    "                    \"entity\": entity_lvl2,  # 保留 B-XXX 或 I-XXX\n",
    "                    \"score\": np.float32(score),\n",
    "                    \"index\": idx,\n",
    "                    \"word\": tokens[idx],\n",
    "                    \"start\": start,\n",
    "                    \"end\": end\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 直接預測2\n",
    "def get_level2_entities_2(model, tokenizer, sentence, label_map_lvl2, choose_loss ):\n",
    "    # 1. Tokenize with offsets\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # 2. Model forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, choose_loss=choose_loss)\n",
    "        logits_lvl1, logits_lvl2 = outputs.logits  # tuple of two tensors\n",
    "\n",
    "    # 3. Predict level 2 labels\n",
    "    preds_lvl2 = torch.argmax(logits_lvl2, dim=2)[0].cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for idx, pred_id in enumerate(preds_lvl2):\n",
    "        token_id = input_ids[0, idx].item()\n",
    "        # 跳過特殊token\n",
    "        if token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "            continue\n",
    "\n",
    "        start, end = offsets[idx]\n",
    "        if start == end:\n",
    "            continue\n",
    "\n",
    "        token_str = tokens[idx]\n",
    "        entity = label_map_lvl2.get(pred_id, \"O\")\n",
    "        if entity != \"O\":\n",
    "            probs = torch.softmax(logits_lvl2[0, idx], dim=0)\n",
    "            score = float(probs[pred_id].cpu().numpy())\n",
    "\n",
    "            results.append({\n",
    "                \"entity\": entity,\n",
    "                \"score\": np.float32(score),\n",
    "                \"index\": idx,\n",
    "                \"word\": token_str,  # 改用 tokenizer 拆出來的 token\n",
    "                \"start\": start,\n",
    "                \"end\": end\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 不考慮O才預測\n",
    "def get_level2_entities_3(model, tokenizer, sentence, label_map_lvl1, label_map_lvl2, choose_loss):\n",
    "    # 1. Tokenize with offsets\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # 2. Model forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, choose_loss=choose_loss)\n",
    "        logits_lvl1, logits_lvl2 = outputs.logits  # tuple of two tensors\n",
    "\n",
    "    # 3. Predict Level 1 & Level 2\n",
    "    preds_lvl1 = torch.argmax(logits_lvl1, dim=2)[0].cpu().numpy()\n",
    "    preds_lvl2 = torch.argmax(logits_lvl2, dim=2)[0].cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for idx, (pred1_id, pred2_id) in enumerate(zip(preds_lvl1, preds_lvl2)):\n",
    "        token_id = input_ids[0, idx].item()\n",
    "        # 跳過特殊 token\n",
    "        if token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "            continue\n",
    "\n",
    "        start, end = offsets[idx]\n",
    "        if start == end:\n",
    "            continue\n",
    "\n",
    "        entity_lvl1 = label_map_lvl1.get(pred1_id, \"O\")\n",
    "        entity_lvl2 = label_map_lvl2.get(pred2_id, \"O\")\n",
    "\n",
    "        # ✅ 僅當 Level 1 不是 \"O\" 才考慮 Level 2 的結果\n",
    "        if entity_lvl1 != \"O\" and entity_lvl2 != \"O\":\n",
    "            probs = torch.softmax(logits_lvl2[0, idx], dim=0)\n",
    "            score = float(probs[pred2_id].cpu().numpy())\n",
    "\n",
    "            results.append({\n",
    "                \"entity\": entity_lvl2,\n",
    "                \"score\": np.float32(score),\n",
    "                \"index\": idx,\n",
    "                \"word\": tokens[idx],  # 使用 tokenizer 拆出來的 token\n",
    "                \"start\": start,\n",
    "                \"end\": end\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 直接預測2\n",
    "def get_level2_entities_normal(model, tokenizer, sentence, label_map):\n",
    "    # 1. Tokenize with offsets\n",
    "    # encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "    # input_ids = encoding[\"input_ids\"]\n",
    "    # attention_mask = encoding[\"attention_mask\"]\n",
    "    # offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "    # tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
    "    input_ids = encoding[\"input_ids\"].to(device)          # 放到 GPU\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)  # 放到 GPU\n",
    "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu())  # token ids 放 CPU 才能用 tokenizer\n",
    "\n",
    "    # 2. Model forward\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, num_labels]\n",
    "\n",
    "\n",
    "    preds = torch.argmax(logits, dim=2)[0].cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for idx, (pred_id, offset) in enumerate(zip(preds, offsets)):\n",
    "        token_id = input_ids[0, idx].item()\n",
    "\n",
    "        # 跳過特殊 token 或無效 offset\n",
    "        if token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
    "            continue\n",
    "\n",
    "        start, end = offset\n",
    "        entity = label_map.get(pred_id, \"O\")\n",
    "\n",
    "        if entity != \"O\":\n",
    "            probs = torch.softmax(logits[0, idx], dim=0)\n",
    "            score = probs[pred_id].item()\n",
    "\n",
    "            results.append({\n",
    "                \"entity\": entity,\n",
    "                \"score\": np.float32(score),\n",
    "                \"index\": idx,\n",
    "                \"word\": tokens[idx],  # 更準確\n",
    "                \"start\": start,\n",
    "                \"end\": end\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obe6qFlyAg0a"
   },
   "outputs": [],
   "source": [
    "def Choose_label_map( name ) :\n",
    "\n",
    "  label_map_BIO = {\n",
    "    0: 'O',\n",
    "    1: 'B-PATIENT', 2: 'I-PATIENT',\n",
    "    3: 'B-DOCTOR', 4: 'I-DOCTOR',\n",
    "    5: 'B-USERNAME', 6: 'I-USERNAME',\n",
    "    7: 'B-FAMILYNAME', 8: 'I-FAMILYNAME',\n",
    "    9: 'B-PERSONALNAME', 10: 'I-PERSONALNAME',\n",
    "    11: 'B-PROFESSION', 12: 'I-PROFESSION',\n",
    "    13: 'B-ROOM', 14: 'I-ROOM',\n",
    "    15: 'B-DEPARTMENT', 16: 'I-DEPARTMENT',\n",
    "    17: 'B-HOSPITAL', 18: 'I-HOSPITAL',\n",
    "    19: 'B-ORGANIZATION', 20: 'I-ORGANIZATION',\n",
    "    21: 'B-STREET', 22: 'I-STREET',\n",
    "    23: 'B-CITY', 24: 'I-CITY',\n",
    "    25: 'B-DISTRICT', 26: 'I-DISTRICT',\n",
    "    27: 'B-COUNTY', 28: 'I-COUNTY',\n",
    "    29: 'B-STATE', 30: 'I-STATE',\n",
    "    31: 'B-COUNTRY', 32: 'I-COUNTRY',\n",
    "    33: 'B-ZIP', 34: 'I-ZIP',\n",
    "    35: 'B-LOCATION-OTHER', 36: 'I-LOCATION-OTHER',\n",
    "    37: 'B-AGE', 38: 'I-AGE',\n",
    "    39: 'B-DATE', 40: 'I-DATE',\n",
    "    41: 'B-TIME', 42: 'I-TIME',\n",
    "    43: 'B-DURATION', 44: 'I-DURATION',\n",
    "    45: 'B-SET', 46: 'I-SET',\n",
    "    47: 'B-PHONE', 48: 'I-PHONE',\n",
    "    49: 'B-FAX', 50: 'I-FAX',\n",
    "    51: 'B-EMAIL', 52: 'I-EMAIL',\n",
    "    53: 'B-URL', 54: 'I-URL',\n",
    "    55: 'B-IPADDRESS', 56: 'I-IPADDRESS',\n",
    "    57: 'B-SOCIAL_SECURITY_NUMBER', 58: 'I-SOCIAL_SECURITY_NUMBER',\n",
    "    59: 'B-MEDICAL_RECORD_NUMBER', 60: 'I-MEDICAL_RECORD_NUMBER',\n",
    "    61: 'B-HEALTH_PLAN_NUMBER', 62: 'I-HEALTH_PLAN_NUMBER',\n",
    "    63: 'B-ACCOUNT_NUMBER', 64: 'I-ACCOUNT_NUMBER',\n",
    "    65: 'B-LICENSE_NUMBER', 66: 'I-LICENSE_NUMBER',\n",
    "    67: 'B-VEHICLE_ID', 68: 'I-VEHICLE_ID',\n",
    "    69: 'B-DEVICE_ID', 70: 'I-DEVICE_ID',\n",
    "    71: 'B-BIOMETRIC_ID', 72: 'I-BIOMETRIC_ID',\n",
    "    73: 'B-ID_NUMBER', 74: 'I-ID_NUMBER',\n",
    "    75: 'B-OTHER', 76: 'I-OTHER'\n",
    "  }\n",
    "\n",
    "  label_map_BIO_CRF = {\n",
    "    0: 'O',\n",
    "    1: 'B-PATIENT', 2: 'I-PATIENT',\n",
    "    3: 'B-DOCTOR', 4: 'I-DOCTOR',\n",
    "    5: 'B-USERNAME', 6: 'I-USERNAME',\n",
    "    7: 'B-FAMILYNAME', 8: 'I-FAMILYNAME',\n",
    "    9: 'B-PERSONALNAME', 10: 'I-PERSONALNAME',\n",
    "    11: 'B-PROFESSION', 12: 'I-PROFESSION',\n",
    "    13: 'B-ROOM', 14: 'I-ROOM',\n",
    "    15: 'B-DEPARTMENT', 16: 'I-DEPARTMENT',\n",
    "    17: 'B-HOSPITAL', 18: 'I-HOSPITAL',\n",
    "    19: 'B-ORGANIZATION', 20: 'I-ORGANIZATION',\n",
    "    21: 'B-STREET', 22: 'I-STREET',\n",
    "    23: 'B-CITY', 24: 'I-CITY',\n",
    "    25: 'B-DISTRICT', 26: 'I-DISTRICT',\n",
    "    27: 'B-COUNTY', 28: 'I-COUNTY',\n",
    "    29: 'B-STATE', 30: 'I-STATE',\n",
    "    31: 'B-COUNTRY', 32: 'I-COUNTRY',\n",
    "    33: 'B-ZIP', 34: 'I-ZIP',\n",
    "    35: 'B-LOCATION-OTHER', 36: 'I-LOCATION-OTHER',\n",
    "    37: 'B-AGE', 38: 'I-AGE',\n",
    "    39: 'B-DATE', 40: 'I-DATE',\n",
    "    41: 'B-TIME', 42: 'I-TIME',\n",
    "    43: 'B-DURATION', 44: 'I-DURATION',\n",
    "    45: 'B-SET', 46: 'I-SET',\n",
    "    47: 'B-PHONE', 48: 'I-PHONE',\n",
    "    49: 'B-FAX', 50: 'I-FAX',\n",
    "    51: 'B-EMAIL', 52: 'I-EMAIL',\n",
    "    53: 'B-URL', 54: 'I-URL',\n",
    "    55: 'B-IPADDRESS', 56: 'I-IPADDRESS',\n",
    "    57: 'B-SOCIAL_SECURITY_NUMBER', 58: 'I-SOCIAL_SECURITY_NUMBER',\n",
    "    59: 'B-MEDICAL_RECORD_NUMBER', 60: 'I-MEDICAL_RECORD_NUMBER',\n",
    "    61: 'B-HEALTH_PLAN_NUMBER', 62: 'I-HEALTH_PLAN_NUMBER',\n",
    "    63: 'B-ACCOUNT_NUMBER', 64: 'I-ACCOUNT_NUMBER',\n",
    "    65: 'B-LICENSE_NUMBER', 66: 'I-LICENSE_NUMBER',\n",
    "    67: 'B-VEHICLE_ID', 68: 'I-VEHICLE_ID',\n",
    "    69: 'B-DEVICE_ID', 70: 'I-DEVICE_ID',\n",
    "    71: 'B-BIOMETRIC_ID', 72: 'I-BIOMETRIC_ID',\n",
    "    73: 'B-ID_NUMBER', 74: 'I-ID_NUMBER',\n",
    "    75: 'B-OTHER', 76: 'I-OTHER',\n",
    "    77: \"IGNORE\"\n",
    "  }\n",
    "\n",
    "  label_map_BIOU = { 0: 'O',\n",
    "    1: 'B-PATIENT', 2: 'I-PATIENT', 3: 'L-PATIENT', 4: 'U-PATIENT',\n",
    "    5: 'B-DOCTOR', 6: 'I-DOCTOR', 7: 'L-DOCTOR', 8: 'U-DOCTOR',\n",
    "    9: 'B-USERNAME', 10: 'I-USERNAME', 11: 'L-USERNAME', 12: 'U-USERNAME',\n",
    "    13: 'B-FAMILYNAME', 14: 'I-FAMILYNAME', 15: 'L-FAMILYNAME', 16: 'U-FAMILYNAME',\n",
    "    17: 'B-PERSONALNAME', 18: 'I-PERSONALNAME', 19: 'L-PERSONALNAME', 20: 'U-PERSONALNAME',\n",
    "    21: 'B-PROFESSION', 22: 'I-PROFESSION', 23: 'L-PROFESSION', 24: 'U-PROFESSION',\n",
    "    25: 'B-ROOM', 26: 'I-ROOM', 27: 'L-ROOM', 28: 'U-ROOM',\n",
    "    29: 'B-DEPARTMENT', 30: 'I-DEPARTMENT', 31: 'L-DEPARTMENT', 32: 'U-DEPARTMENT',\n",
    "    33: 'B-HOSPITAL', 34: 'I-HOSPITAL', 35: 'L-HOSPITAL', 36: 'U-HOSPITAL',\n",
    "    37: 'B-ORGANIZATION', 38: 'I-ORGANIZATION', 39: 'L-ORGANIZATION', 40: 'U-ORGANIZATION',\n",
    "    41: 'B-STREET', 42: 'I-STREET', 43: 'L-STREET', 44: 'U-STREET',\n",
    "    45: 'B-CITY', 46: 'I-CITY', 47: 'L-CITY', 48: 'U-CITY',\n",
    "    49: 'B-DISTRICT', 50: 'I-DISTRICT', 51: 'L-DISTRICT', 52: 'U-DISTRICT',\n",
    "    53: 'B-COUNTY', 54: 'I-COUNTY', 55: 'L-COUNTY', 56: 'U-COUNTY',\n",
    "    57: 'B-STATE', 58: 'I-STATE', 59: 'L-STATE', 60: 'U-STATE',\n",
    "    61: 'B-COUNTRY', 62: 'I-COUNTRY', 63: 'L-COUNTRY', 64: 'U-COUNTRY',\n",
    "    65: 'B-ZIP', 66: 'I-ZIP', 67: 'L-ZIP', 68: 'U-ZIP',\n",
    "    69: 'B-LOCATION-OTHER', 70: 'I-LOCATION-OTHER', 71: 'L-LOCATION-OTHER', 72: 'U-LOCATION-OTHER',\n",
    "    73: 'B-AGE', 74: 'I-AGE', 75: 'L-AGE', 76: 'U-AGE',\n",
    "    77: 'B-DATE', 78: 'I-DATE', 79: 'L-DATE', 80: 'U-DATE',\n",
    "    81: 'B-TIME', 82: 'I-TIME', 83: 'L-TIME', 84: 'U-TIME',\n",
    "    85: 'B-DURATION', 86: 'I-DURATION', 87: 'L-DURATION', 88: 'U-DURATION',\n",
    "    89: 'B-SET', 90: 'I-SET', 91: 'L-SET', 92: 'U-SET',\n",
    "    93: 'B-PHONE', 94: 'I-PHONE', 95: 'L-PHONE', 96: 'U-PHONE',\n",
    "    97: 'B-FAX', 98: 'I-FAX', 99: 'L-FAX', 100: 'U-FAX',\n",
    "    101: 'B-EMAIL', 102: 'I-EMAIL', 103: 'L-EMAIL', 104: 'U-EMAIL',\n",
    "    105: 'B-URL', 106: 'I-URL', 107: 'L-URL', 108: 'U-URL',\n",
    "    109: 'B-IPADDRESS', 110: 'I-IPADDRESS', 111: 'L-IPADDRESS', 112: 'U-IPADDRESS',\n",
    "    113: 'B-SOCIAL_SECURITY_NUMBER', 114: 'I-SOCIAL_SECURITY_NUMBER', 115: 'L-SOCIAL_SECURITY_NUMBER', 116: 'U-SOCIAL_SECURITY_NUMBER',\n",
    "    117: 'B-MEDICAL_RECORD_NUMBER', 118: 'I-MEDICAL_RECORD_NUMBER', 119: 'L-MEDICAL_RECORD_NUMBER', 120: 'U-MEDICAL_RECORD_NUMBER',\n",
    "    121: 'B-HEALTH_PLAN_NUMBER', 122: 'I-HEALTH_PLAN_NUMBER', 123: 'L-HEALTH_PLAN_NUMBER', 124: 'U-HEALTH_PLAN_NUMBER',\n",
    "    125: 'B-ACCOUNT_NUMBER', 126: 'I-ACCOUNT_NUMBER', 127: 'L-ACCOUNT_NUMBER', 128: 'U-ACCOUNT_NUMBER',\n",
    "    129: 'B-LICENSE_NUMBER', 130: 'I-LICENSE_NUMBER', 131: 'L-LICENSE_NUMBER', 132: 'U-LICENSE_NUMBER',\n",
    "    133: 'B-VEHICLE_ID', 134: 'I-VEHICLE_ID', 135: 'L-VEHICLE_ID', 136: 'U-VEHICLE_ID',\n",
    "    137: 'B-DEVICE_ID', 138: 'I-DEVICE_ID', 139: 'L-DEVICE_ID', 140: 'U-DEVICE_ID',\n",
    "    141: 'B-BIOMETRIC_ID', 142: 'I-BIOMETRIC_ID', 143: 'L-BIOMETRIC_ID', 144: 'U-BIOMETRIC_ID',\n",
    "    145: 'B-ID_NUMBER', 146: 'I-ID_NUMBER', 147: 'L-ID_NUMBER', 148: 'U-ID_NUMBER',\n",
    "    149: 'B-OTHER', 150: 'I-OTHER', 151: 'L-OTHER', 152: 'U-OTHER'\n",
    "    }\n",
    "\n",
    "\n",
    "  label_map_BIOU_CRF = { 0: 'O',\n",
    "    1: 'B-PATIENT', 2: 'I-PATIENT', 3: 'L-PATIENT', 4: 'U-PATIENT',\n",
    "    5: 'B-DOCTOR', 6: 'I-DOCTOR', 7: 'L-DOCTOR', 8: 'U-DOCTOR',\n",
    "    9: 'B-USERNAME', 10: 'I-USERNAME', 11: 'L-USERNAME', 12: 'U-USERNAME',\n",
    "    13: 'B-FAMILYNAME', 14: 'I-FAMILYNAME', 15: 'L-FAMILYNAME', 16: 'U-FAMILYNAME',\n",
    "    17: 'B-PERSONALNAME', 18: 'I-PERSONALNAME', 19: 'L-PERSONALNAME', 20: 'U-PERSONALNAME',\n",
    "    21: 'B-PROFESSION', 22: 'I-PROFESSION', 23: 'L-PROFESSION', 24: 'U-PROFESSION',\n",
    "    25: 'B-ROOM', 26: 'I-ROOM', 27: 'L-ROOM', 28: 'U-ROOM',\n",
    "    29: 'B-DEPARTMENT', 30: 'I-DEPARTMENT', 31: 'L-DEPARTMENT', 32: 'U-DEPARTMENT',\n",
    "    33: 'B-HOSPITAL', 34: 'I-HOSPITAL', 35: 'L-HOSPITAL', 36: 'U-HOSPITAL',\n",
    "    37: 'B-ORGANIZATION', 38: 'I-ORGANIZATION', 39: 'L-ORGANIZATION', 40: 'U-ORGANIZATION',\n",
    "    41: 'B-STREET', 42: 'I-STREET', 43: 'L-STREET', 44: 'U-STREET',\n",
    "    45: 'B-CITY', 46: 'I-CITY', 47: 'L-CITY', 48: 'U-CITY',\n",
    "    49: 'B-DISTRICT', 50: 'I-DISTRICT', 51: 'L-DISTRICT', 52: 'U-DISTRICT',\n",
    "    53: 'B-COUNTY', 54: 'I-COUNTY', 55: 'L-COUNTY', 56: 'U-COUNTY',\n",
    "    57: 'B-STATE', 58: 'I-STATE', 59: 'L-STATE', 60: 'U-STATE',\n",
    "    61: 'B-COUNTRY', 62: 'I-COUNTRY', 63: 'L-COUNTRY', 64: 'U-COUNTRY',\n",
    "    65: 'B-ZIP', 66: 'I-ZIP', 67: 'L-ZIP', 68: 'U-ZIP',\n",
    "    69: 'B-LOCATION-OTHER', 70: 'I-LOCATION-OTHER', 71: 'L-LOCATION-OTHER', 72: 'U-LOCATION-OTHER',\n",
    "    73: 'B-AGE', 74: 'I-AGE', 75: 'L-AGE', 76: 'U-AGE',\n",
    "    77: 'B-DATE', 78: 'I-DATE', 79: 'L-DATE', 80: 'U-DATE',\n",
    "    81: 'B-TIME', 82: 'I-TIME', 83: 'L-TIME', 84: 'U-TIME',\n",
    "    85: 'B-DURATION', 86: 'I-DURATION', 87: 'L-DURATION', 88: 'U-DURATION',\n",
    "    89: 'B-SET', 90: 'I-SET', 91: 'L-SET', 92: 'U-SET',\n",
    "    93: 'B-PHONE', 94: 'I-PHONE', 95: 'L-PHONE', 96: 'U-PHONE',\n",
    "    97: 'B-FAX', 98: 'I-FAX', 99: 'L-FAX', 100: 'U-FAX',\n",
    "    101: 'B-EMAIL', 102: 'I-EMAIL', 103: 'L-EMAIL', 104: 'U-EMAIL',\n",
    "    105: 'B-URL', 106: 'I-URL', 107: 'L-URL', 108: 'U-URL',\n",
    "    109: 'B-IPADDRESS', 110: 'I-IPADDRESS', 111: 'L-IPADDRESS', 112: 'U-IPADDRESS',\n",
    "    113: 'B-SOCIAL_SECURITY_NUMBER', 114: 'I-SOCIAL_SECURITY_NUMBER', 115: 'L-SOCIAL_SECURITY_NUMBER', 116: 'U-SOCIAL_SECURITY_NUMBER',\n",
    "    117: 'B-MEDICAL_RECORD_NUMBER', 118: 'I-MEDICAL_RECORD_NUMBER', 119: 'L-MEDICAL_RECORD_NUMBER', 120: 'U-MEDICAL_RECORD_NUMBER',\n",
    "    121: 'B-HEALTH_PLAN_NUMBER', 122: 'I-HEALTH_PLAN_NUMBER', 123: 'L-HEALTH_PLAN_NUMBER', 124: 'U-HEALTH_PLAN_NUMBER',\n",
    "    125: 'B-ACCOUNT_NUMBER', 126: 'I-ACCOUNT_NUMBER', 127: 'L-ACCOUNT_NUMBER', 128: 'U-ACCOUNT_NUMBER',\n",
    "    129: 'B-LICENSE_NUMBER', 130: 'I-LICENSE_NUMBER', 131: 'L-LICENSE_NUMBER', 132: 'U-LICENSE_NUMBER',\n",
    "    133: 'B-VEHICLE_ID', 134: 'I-VEHICLE_ID', 135: 'L-VEHICLE_ID', 136: 'U-VEHICLE_ID',\n",
    "    137: 'B-DEVICE_ID', 138: 'I-DEVICE_ID', 139: 'L-DEVICE_ID', 140: 'U-DEVICE_ID',\n",
    "    141: 'B-BIOMETRIC_ID', 142: 'I-BIOMETRIC_ID', 143: 'L-BIOMETRIC_ID', 144: 'U-BIOMETRIC_ID',\n",
    "    145: 'B-ID_NUMBER', 146: 'I-ID_NUMBER', 147: 'L-ID_NUMBER', 148: 'U-ID_NUMBER',\n",
    "    149: 'B-OTHER', 150: 'I-OTHER', 151: 'L-OTHER', 152: 'U-OTHER', 153: \"IGNORE\"\n",
    "    }\n",
    "\n",
    "  label_map_big = {\n",
    "        0: 'O',\n",
    "        1: 'B-NAME', 2: 'I-NAME',\n",
    "        3: 'B-OCCUPATION', 4: 'I-OCCUPATION',\n",
    "        5: 'B-LOCATION', 6: 'I-LOCATION',\n",
    "        7: 'B-AGE', 8: 'I-AGE',\n",
    "        9: 'B-DATE', 10: 'I-DATE',\n",
    "        11: 'B-CONTACT_INFORMATION', 12:'I-CONTACT_INFORMATION',\n",
    "        13: 'B-IDENTIFIERS', 14: 'I-IDENTIFIERS',\n",
    "        15: 'B-OTHER', 16: 'I-OTHER'\n",
    "    }\n",
    "\n",
    "\n",
    "  if name == \"BIO\":\n",
    "    return label_map_BIO\n",
    "  elif name == \"BIOU\":\n",
    "    return label_map_BIOU\n",
    "  elif name == \"BIG\":\n",
    "    return label_map_big\n",
    "  elif name == \"BIO_CRF\":\n",
    "    return label_map_BIO_CRF\n",
    "  elif name == \"BIOU_CRF\":\n",
    "    return label_map_BIOU_CRF\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "name = \"BIO\"\n",
    "BIO_label_map = Choose_label_map( name )\n",
    "\n",
    "new_label2id_BIO = {v: k for k, v in BIO_label_map.items()}\n",
    "new_id2label_BIO = BIO_label_map\n",
    "\n",
    "\n",
    "name = \"BIOU\"\n",
    "BIOU_label_map = Choose_label_map( name )\n",
    "\n",
    "new_label2id_BIOU = {v: k for k, v in BIOU_label_map.items()}\n",
    "new_id2label_BIOU = BIOU_label_map\n",
    "\n",
    "\n",
    "name =\"BIG\"\n",
    "BIG_label_map = Choose_label_map( name )\n",
    "\n",
    "new_label2id_BIG = {v: k for k, v in BIG_label_map.items()}\n",
    "new_id2label_BIG = BIG_label_map\n",
    "\n",
    "\n",
    "name =\"BIO_CRF\"\n",
    "BIO_CRF_label_map = Choose_label_map( name )\n",
    "\n",
    "new_label2id_BIO_CRF = {v: k for k, v in BIO_CRF_label_map.items()}\n",
    "new_id2label_BIO_CRF = BIO_CRF_label_map\n",
    "\n",
    "\n",
    "name =\"BIOU_CRF\"\n",
    "BIOU_CRF_label_map = Choose_label_map( name )\n",
    "\n",
    "new_label2id_BIOU_CRF = {v: k for k, v in BIOU_CRF_label_map.items()}\n",
    "new_id2label_BIOU_CRF = BIOU_CRF_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U53_Q6DlAmfk"
   },
   "outputs": [],
   "source": [
    "def Process_Predict_Ner_BIOUL(pre):\n",
    "    answer_list = []\n",
    "    current_entity = None\n",
    "    current_word = \"\"\n",
    "    start_pos = None\n",
    "    end_pos = None\n",
    "\n",
    "    for dic in pre:\n",
    "        entity_type = dic['entity']\n",
    "        raw_word = dic['word']\n",
    "        word = raw_word.replace(\"▁\", \"\")\n",
    "        token_start = dic.get('start')\n",
    "        token_end = dic.get('end')\n",
    "        has_space = raw_word.startswith(\"▁\")\n",
    "\n",
    "        if entity_type.startswith(\"B-\"):\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = entity_type.replace(\"B-\", \"\")\n",
    "            current_word = word\n",
    "            start_pos = token_start\n",
    "            end_pos = token_end\n",
    "\n",
    "        elif entity_type.startswith(\"I-\"):\n",
    "            ent = entity_type.replace(\"I-\", \"\")\n",
    "            if current_entity == ent:\n",
    "                if has_space:\n",
    "                    current_word += \" \" + word\n",
    "                else:\n",
    "                    current_word += word\n",
    "                end_pos = token_end\n",
    "            else:\n",
    "                if current_entity and current_word:\n",
    "                    answer_list.append({\n",
    "                        \"entity\": current_entity,\n",
    "                        \"word\": current_word,\n",
    "                        \"start\": start_pos,\n",
    "                        \"end\": end_pos\n",
    "                    })\n",
    "                current_entity = ent\n",
    "                current_word = word\n",
    "                start_pos = token_start\n",
    "                end_pos = token_end\n",
    "\n",
    "        elif entity_type.startswith(\"L-\"):\n",
    "            ent = entity_type.replace(\"L-\", \"\")\n",
    "            if current_entity == ent:\n",
    "                if has_space:\n",
    "                    current_word += \" \" + word\n",
    "                else:\n",
    "                    current_word += word\n",
    "                end_pos = token_end\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "                current_entity = None\n",
    "                current_word = \"\"\n",
    "                start_pos = None\n",
    "                end_pos = None\n",
    "            else:\n",
    "                # 如果之前的 entity 沒接上，當作獨立實體處理\n",
    "                answer_list.append({\n",
    "                    \"entity\": ent,\n",
    "                    \"word\": word,\n",
    "                    \"start\": token_start,\n",
    "                    \"end\": token_end\n",
    "                })\n",
    "                current_entity = None\n",
    "                current_word = \"\"\n",
    "                start_pos = None\n",
    "                end_pos = None\n",
    "\n",
    "        elif entity_type.startswith(\"U-\"):\n",
    "            ent = entity_type.replace(\"U-\", \"\")\n",
    "            answer_list.append({\n",
    "                \"entity\": ent,\n",
    "                \"word\": word,\n",
    "                \"start\": token_start,\n",
    "                \"end\": token_end\n",
    "            })\n",
    "\n",
    "            current_entity = None\n",
    "            current_word = \"\"\n",
    "            start_pos = None\n",
    "            end_pos = None\n",
    "\n",
    "        else:  # O\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = None\n",
    "            current_word = \"\"\n",
    "            start_pos = None\n",
    "            end_pos = None\n",
    "\n",
    "    # 收尾\n",
    "    if current_entity and current_word:\n",
    "        answer_list.append({\n",
    "            \"entity\": current_entity,\n",
    "            \"word\": current_word,\n",
    "            \"start\": start_pos,\n",
    "            \"end\": end_pos\n",
    "        })\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "\n",
    "def Process_Predict_Ner(pre):\n",
    "    answer_list = []\n",
    "    current_entity = None\n",
    "    current_word = \"\"\n",
    "    start_pos = None\n",
    "    end_pos = None\n",
    "\n",
    "    for dic in pre:\n",
    "        entity_type = dic['entity']\n",
    "        raw_word = dic['word']\n",
    "        word = raw_word.replace(\"▁\", \"\")\n",
    "        token_start = dic.get('start')\n",
    "        token_end = dic.get('end')\n",
    "        has_space = raw_word.startswith(\"▁\")\n",
    "\n",
    "        if entity_type.startswith(\"B-\"):\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = entity_type.replace(\"B-\", \"\")\n",
    "            current_word = word\n",
    "            start_pos = token_start\n",
    "            end_pos = token_end\n",
    "\n",
    "        elif entity_type.startswith(\"I-\"):\n",
    "            ent = entity_type.replace(\"I-\", \"\")\n",
    "            if current_entity == ent:\n",
    "                if has_space:\n",
    "                    current_word += \" \" + word\n",
    "                else:\n",
    "                    current_word += word\n",
    "                end_pos = token_end\n",
    "            else:\n",
    "                if current_entity and current_word:\n",
    "                    answer_list.append({\n",
    "                        \"entity\": current_entity,\n",
    "                        \"word\": current_word,\n",
    "                        \"start\": start_pos,\n",
    "                        \"end\": end_pos\n",
    "                    })\n",
    "                current_entity = ent\n",
    "                current_word = word\n",
    "                start_pos = token_start\n",
    "                end_pos = token_end\n",
    "\n",
    "        else:  # O\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = None\n",
    "            current_word = \"\"\n",
    "            start_pos = None\n",
    "            end_pos = None\n",
    "\n",
    "    # 收尾\n",
    "    if current_entity and current_word:\n",
    "        answer_list.append({\n",
    "            \"entity\": current_entity,\n",
    "            \"word\": current_word,\n",
    "            \"start\": start_pos,\n",
    "            \"end\": end_pos\n",
    "        })\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "def Process_Predict_level(pre):\n",
    "\n",
    "    answer_list = []\n",
    "    current_entity = None\n",
    "    current_word = \"\"\n",
    "    start_pos = None\n",
    "    end_pos = None\n",
    "\n",
    "    for dic in pre:\n",
    "        entity_type = dic['entity']\n",
    "        raw_word = dic['word']\n",
    "        word = raw_word.replace(\"▁\", \"\")\n",
    "        token_start = dic.get('start')\n",
    "        token_end = dic.get('end')\n",
    "        has_space = raw_word.startswith(\"▁\")\n",
    "\n",
    "        if entity_type.startswith(\"B-\"):\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = entity_type.replace(\"B-\", \"\")\n",
    "            current_word = word\n",
    "            start_pos = token_start\n",
    "            end_pos = token_end\n",
    "\n",
    "        elif entity_type.startswith(\"I-\"):\n",
    "            ent = entity_type.replace(\"I-\", \"\")\n",
    "            if current_entity == ent:\n",
    "                if has_space:\n",
    "                    current_word += \" \" + word\n",
    "                else:\n",
    "                    current_word += word\n",
    "                end_pos = token_end\n",
    "            else:\n",
    "                if current_entity and current_word:\n",
    "                    answer_list.append({\n",
    "                        \"entity\": current_entity,\n",
    "                        \"word\": current_word,\n",
    "                        \"start\": start_pos,\n",
    "                        \"end\": end_pos\n",
    "                    })\n",
    "                current_entity = ent\n",
    "                current_word = word\n",
    "                start_pos = token_start\n",
    "                end_pos = token_end\n",
    "\n",
    "        else:  # O\n",
    "            if current_entity and current_word:\n",
    "                answer_list.append({\n",
    "                    \"entity\": current_entity,\n",
    "                    \"word\": current_word,\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos\n",
    "                })\n",
    "            current_entity = None\n",
    "            current_word = \"\"\n",
    "            start_pos = None\n",
    "            end_pos = None\n",
    "\n",
    "    # 收尾\n",
    "    if current_entity and current_word:\n",
    "        answer_list.append({\n",
    "            \"entity\": current_entity,\n",
    "            \"word\": current_word,\n",
    "            \"start\": start_pos,\n",
    "            \"end\": end_pos\n",
    "        })\n",
    "\n",
    "    return answer_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-cbhfUt6TYf"
   },
   "source": [
    "# **CRF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCaMcCld6VTY",
    "outputId": "54c93ef0-ca81-4ce8-8d0a-4fa1bd669e67"
   },
   "outputs": [],
   "source": [
    "!pip install torchcrf\n",
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17Dzgbc05zB4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from transformers import XLMRobertaConfig, XLMRobertaModel, XLMRobertaPreTrainedModel\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import XLMRobertaForTokenClassification\n",
    "\n",
    "\n",
    "\n",
    "class XLMRobertaWithCRF(XLMRobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # 用 from_pretrained 來載入預訓練權重\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", config=config, token=access_token)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            mask = attention_mask.bool()\n",
    "            mask[:, 0] = True  # 確保第一token是有效的mask\n",
    "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=emissions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PplCzk-26Zim"
   },
   "source": [
    "# **FOCAL LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKouwFQ-vHVP"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForTokenClassification\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch.nn.functional as F # Import F\n",
    "\n",
    "\n",
    "\n",
    "class XLMRobertaForTokenClassificationWithFocalLoss(XLMRobertaForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.ignore_index = config.label2id.get(\"0\", -100) if hasattr(config, \"label2id\") else -100\n",
    "        self.gamma = 2.0  # focal loss gamma\n",
    "\n",
    "    def compute_focal_loss(self, logits, targets):\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        probs = torch.exp(log_probs)\n",
    "\n",
    "        targets = targets.view(-1)\n",
    "        log_probs = log_probs.view(-1, log_probs.size(-1))\n",
    "        probs = probs.view(-1, probs.size(-1))\n",
    "\n",
    "        mask = targets != self.ignore_index\n",
    "        targets = targets[mask]\n",
    "        log_probs = log_probs[mask] # Apply mask to log_probs\n",
    "        probs = probs[mask] # Apply mask to probs\n",
    "\n",
    "\n",
    "        if targets.numel() == 0:\n",
    "            return torch.tensor(0.0, dtype=logits.dtype, device=logits.device)\n",
    "\n",
    "        focal_weight = (1 - probs.gather(1, targets.unsqueeze(1)).squeeze()) ** self.gamma\n",
    "        loss = -focal_weight * log_probs.gather(1, targets.unsqueeze(1)).squeeze()\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, ):\n",
    "        # 預測\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,  # 我們自己處理 loss\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.compute_focal_loss(logits, labels)\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn-yWsFh6eBG"
   },
   "source": [
    "# **LEVEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ARe7ksGBDl5"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaPreTrainedModel, XLMRobertaModel\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class XLMRobertaForHierarchicalTokenClassification(XLMRobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels_lvl1 = getattr(config, \"num_labels_lvl1\", None)\n",
    "        self.num_labels_lvl2 = getattr(config, \"num_labels_lvl2\", None)\n",
    "\n",
    "        self.roberta = XLMRobertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.classifier_lvl1 = nn.Linear(config.hidden_size, self.num_labels_lvl1)\n",
    "        self.classifier_lvl2 = nn.Linear(config.hidden_size, self.num_labels_lvl2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    ## loss\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels_lvl1=None, labels_lvl2=None, choose_loss = 1 ):\n",
    "\n",
    "        ## loss 1\n",
    "        if choose_loss == 1:\n",
    "          outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "          sequence_output = self.dropout(outputs[0])\n",
    "\n",
    "          logits_lvl1 = self.classifier_lvl1(sequence_output)\n",
    "          logits_lvl2 = self.classifier_lvl2(sequence_output)\n",
    "\n",
    "          loss = None\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          if labels_lvl1 is not None and labels_lvl2 is not None:\n",
    "              loss1 = loss_fct(logits_lvl1.view(-1, self.num_labels_lvl1), labels_lvl1.view(-1))\n",
    "              loss2 = loss_fct(logits_lvl2.view(-1, self.num_labels_lvl2), labels_lvl2.view(-1))\n",
    "              loss = loss1 + loss2\n",
    "\n",
    "          return TokenClassifierOutput(\n",
    "              loss=loss,\n",
    "              logits=(logits_lvl1, logits_lvl2)  # ⚠️ 注意這裡傳 tuple\n",
    "          )\n",
    "\n",
    "        ## loss 2\n",
    "        elif choose_loss == 2:\n",
    "\n",
    "          outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "          sequence_output = self.dropout(outputs[0])\n",
    "\n",
    "          logits_lvl1 = self.classifier_lvl1(sequence_output)\n",
    "          logits_lvl2 = self.classifier_lvl2(sequence_output)\n",
    "\n",
    "          loss = None\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "          if labels_lvl1 is not None and labels_lvl2 is not None:\n",
    "              # Level 1 loss\n",
    "              loss1 = loss_fct(logits_lvl1.view(-1, self.num_labels_lvl1), labels_lvl1.view(-1))\n",
    "\n",
    "              # Level 2 loss masking: 只在 Level 1 不是 O 的 token 上計算\n",
    "              active_mask = (labels_lvl1.view(-1) != 0)  # 假設 \"O\" 是 label id 0\n",
    "              active_logits_lvl2 = logits_lvl2.view(-1, self.num_labels_lvl2)[active_mask]\n",
    "              active_labels_lvl2 = labels_lvl2.view(-1)[active_mask]\n",
    "\n",
    "              if active_labels_lvl2.numel() > 0:\n",
    "                  loss2 = loss_fct(active_logits_lvl2, active_labels_lvl2)\n",
    "              else:\n",
    "                  loss2 = 0.0  # 沒有合法 token，不加 loss2\n",
    "\n",
    "              loss = loss1 + loss2\n",
    "\n",
    "          return TokenClassifierOutput(\n",
    "              loss=loss,\n",
    "              logits=(logits_lvl1, logits_lvl2)  # 注意：回傳 tuple\n",
    "          )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Jtf_DItqNn"
   },
   "source": [
    "# **weight_class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e7YF9CPtpOX"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForTokenClassification\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch.nn.functional as F # Import F\n",
    "\n",
    "\n",
    "\n",
    "from transformers import XLMRobertaForTokenClassification, AutoConfig\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "\n",
    "\n",
    "class XLMRobertaForTokenClassificationWithClassWeight(XLMRobertaForTokenClassification):\n",
    "    def __init__(self, config, class_weights=None):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.loss_fct = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "        else:\n",
    "            self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,  # 必須保留\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.contiguous().view(-1, self.num_labels), labels.contiguous().view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtiVzVB66gZp"
   },
   "source": [
    "# **預測**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yeisj0v176bD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_overlap(pred_start, pred_end, gt_start, gt_end):\n",
    "    \"\"\"計算兩個時間區間的重疊長度\"\"\"\n",
    "    overlap_start = max(pred_start, gt_start)\n",
    "    overlap_end = min(pred_end, gt_end)\n",
    "    overlap = max(0, overlap_end - overlap_start)\n",
    "    return overlap\n",
    "\n",
    "def evaluate_task2(prediction_file, ground_truth_file):\n",
    "    # 讀取預測和真實標籤數據\n",
    "    pred_df = pd.read_csv(prediction_file, sep='\\t', header=None,\n",
    "                         names=['id', 'type', 'start', 'end', 'content'])\n",
    "    gt_df = pd.read_csv(ground_truth_file, sep='\\t', header=None,\n",
    "                       names=['id', 'type', 'start', 'end', 'content'])\n",
    "\n",
    "    # 獲取所有獨特的SHI類型\n",
    "    all_types = sorted(set(gt_df['type'].unique()) | set(pred_df['type'].unique()))\n",
    "\n",
    "    # 初始化每種類型的指標\n",
    "    metrics = {shi_type: {'tp': 0, 'fp': 0, 'fn': 0} for shi_type in all_types}\n",
    "\n",
    "    # 按音頻ID分組處理\n",
    "    unique_ids = sorted(set(gt_df['id'].unique()) | set(pred_df['id'].unique()))\n",
    "\n",
    "    for audio_id in unique_ids:\n",
    "        gt_records = gt_df[gt_df['id'] == audio_id].copy()\n",
    "        pred_records = pred_df[pred_df['id'] == audio_id].copy()\n",
    "\n",
    "        # 初始化匹配矩陣來追蹤已處理的預測和真實標籤\n",
    "        gt_matched = [False] * len(gt_records)\n",
    "        pred_matched = [False] * len(pred_records)\n",
    "\n",
    "        # 計算True Positives和部分False Positives/False Negatives\n",
    "        for i, pred_row in enumerate(pred_records.itertuples()):\n",
    "            pred_type = pred_row.type\n",
    "            pred_start = pred_row.start\n",
    "            pred_end = pred_row.end\n",
    "            pred_duration = pred_end - pred_start\n",
    "\n",
    "            best_overlap = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            # 找到與當前預測重疊最大的真實標籤\n",
    "            for j, gt_row in enumerate(gt_records.itertuples()):\n",
    "                if gt_row.type != pred_type:\n",
    "                    continue\n",
    "\n",
    "                overlap = calculate_overlap(pred_start, pred_end, gt_row.start, gt_row.end)\n",
    "                if overlap > best_overlap:\n",
    "                    best_overlap = overlap\n",
    "                    best_gt_idx = j\n",
    "\n",
    "            if best_gt_idx >= 0:  # 找到部分匹配\n",
    "                gt_row = gt_records.iloc[best_gt_idx]\n",
    "                gt_duration = gt_row.end - gt_row.start\n",
    "\n",
    "                # 計算 True Positive\n",
    "                metrics[pred_type]['tp'] += best_overlap\n",
    "\n",
    "                # 計算 False Positive (對於部分匹配，類型相同)\n",
    "                metrics[pred_type]['fp'] += pred_duration - best_overlap\n",
    "\n",
    "                # 計算 False Negative (對於部分匹配，類型相同)\n",
    "                metrics[pred_type]['fn'] += gt_duration - best_overlap\n",
    "\n",
    "                # 標記已處理\n",
    "                gt_matched[best_gt_idx] = True\n",
    "                pred_matched[i] = True\n",
    "            else:\n",
    "                # 完全不匹配或者類型不同：整個預測為False Positive\n",
    "                metrics[pred_type]['fp'] += pred_duration\n",
    "\n",
    "        # 處理未匹配的真實標籤 (False Negatives)\n",
    "        for j, matched in enumerate(gt_matched):\n",
    "            if not matched:\n",
    "                gt_row = gt_records.iloc[j]\n",
    "                gt_type = gt_row.type\n",
    "                gt_duration = gt_row.end - gt_row.start\n",
    "                metrics[gt_type]['fn'] += gt_duration\n",
    "\n",
    "        # 處理與類型不同的預測 (False Positives)\n",
    "        for i, (matched, pred_row) in enumerate(zip(pred_matched, pred_records.itertuples())):\n",
    "            if matched:\n",
    "                continue\n",
    "\n",
    "            # 檢查是否有與其他類型匹配\n",
    "            pred_type = pred_row.type\n",
    "            pred_start = pred_row.start\n",
    "            pred_end = pred_row.end\n",
    "            pred_duration = pred_end - pred_start\n",
    "\n",
    "            for gt_row in gt_records.itertuples():\n",
    "                if gt_row.type == pred_type:\n",
    "                    continue  # 已在之前的步驟中處理過\n",
    "\n",
    "                overlap = calculate_overlap(pred_start, pred_end, gt_row.start, gt_row.end)\n",
    "                if overlap > 0:\n",
    "                    # 類型不匹配但時間重疊：整個預測為False Positive\n",
    "                    metrics[pred_type]['fp'] += pred_duration\n",
    "                    break\n",
    "\n",
    "    # 計算每種類型的Precision, Recall和F1\n",
    "    f1_scores = []\n",
    "    for shi_type in all_types:\n",
    "        m = metrics[shi_type]\n",
    "        precision = m['tp'] / (m['tp'] + m['fp']) if (m['tp'] + m['fp']) > 0 else 0\n",
    "        recall = m['tp'] / (m['tp'] + m['fn']) if (m['tp'] + m['fn']) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"類型 {shi_type}:\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1: {f1:.4f}\")\n",
    "        print(f\"  TP: {m['tp']:.2f}, FP: {m['fp']:.2f}, FN: {m['fn']:.2f}\")\n",
    "        print()\n",
    "\n",
    "    # 計算宏平均F1\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    print(f\"Macro-Average F1: {macro_f1:.4f}\")\n",
    "\n",
    "    return macro_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZB5bm6-4Tcu"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_name = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "# 載入 task1 資料：{id: sentence}\n",
    "def load_task1(filepath):\n",
    "    id2text = {}\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) == 2:\n",
    "                id_, text = parts\n",
    "                id2text[id_] = text\n",
    "    return id2text\n",
    "\n",
    "# 載入 task2 為 DataFrame\n",
    "def load_task2(filepath):\n",
    "    rows = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # 先去除行首尾空白與換行\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 5:\n",
    "                parts = [p.strip() for p in parts]  # 再去每個欄位的頭尾空白\n",
    "                rows.append(parts)\n",
    "    df = pd.DataFrame(rows, columns=[\"id\", \"type\", \"start_time\", \"end_time\", \"content\"])\n",
    "    return df\n",
    "\n",
    "def find_nth_occurrence(text, substring, n):\n",
    "    start = -1\n",
    "    for _ in range(n):\n",
    "        start = text.find(substring, start + 1)\n",
    "        if start == -1:\n",
    "            return -1\n",
    "    return start\n",
    "\n",
    "\n",
    "def map_entities_to_char_indices_with_duplicates(task1_file, task2_file, output_file=None):\n",
    "    id2text = load_task1(task1_file)\n",
    "    df_task2 = load_task2(task2_file)\n",
    "\n",
    "    # 用來記錄每個 (id, entity_text) 出現次數\n",
    "    occurrence_counter = {}\n",
    "\n",
    "    results = []\n",
    "    for row in df_task2.itertuples():\n",
    "        tid = str(row.id)\n",
    "        entity_text = row.content.strip()\n",
    "        entity_type = row.type\n",
    "\n",
    "        if tid not in id2text:\n",
    "            results.append([tid, entity_type, -1, -1, entity_text])\n",
    "            continue\n",
    "\n",
    "        sentence = id2text[tid]\n",
    "\n",
    "        key = (tid, entity_text)\n",
    "        occurrence_counter[key] = occurrence_counter.get(key, 0) + 1\n",
    "        nth = occurrence_counter[key]\n",
    "\n",
    "        start_char = find_nth_occurrence(sentence, entity_text, nth)\n",
    "        if start_char == -1:\n",
    "            start_char, end_char = -1, -1\n",
    "        else:\n",
    "            end_char = start_char + len(entity_text)\n",
    "\n",
    "        results.append([tid, entity_type, start_char, end_char, entity_text])\n",
    "\n",
    "    if output_file:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for r in results:\n",
    "                f.write(\"\\t\".join(map(str, r)) + \"\\n\")\n",
    "\n",
    "    return pd.DataFrame(results, columns=['id', 'type', 'start_char', 'end_char', 'content'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfPaKiJv4UPr"
   },
   "outputs": [],
   "source": [
    "df_result = map_entities_to_char_indices_with_duplicates(\n",
    "    model_test_task1_data_path_txt,\n",
    "    model_test_task2_data_path_txt,\n",
    "    \"./entity_token_indices.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNojFY6YMEAu"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tabulate import tabulate  # pip install tabulate\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 測試task1資料\n",
    "with open( model_test_task1_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
    "  test_data = f.readlines()\n",
    "\n",
    "\n",
    "model_name = [\n",
    "           model_1_path\n",
    "             ]  # 你想測試的模型路徑列表\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_id in model_name:\n",
    "    choose_loss = 0\n",
    "    answer = \"\"\n",
    "\n",
    "    if \"level\" in model_id:\n",
    "        if \"loss_1\" in model_id:\n",
    "            choose_loss = 1\n",
    "        elif \"loss_2\" in model_id:\n",
    "            choose_loss = 2\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "        model = XLMRobertaForHierarchicalTokenClassification.from_pretrained(model_id)\n",
    "\n",
    "        new_label2id = new_label2id_BIO\n",
    "        new_id2label = new_id2label_BIO\n",
    "        label_map = BIO_label_map\n",
    "        label_map_big = BIG_label_map\n",
    "\n",
    "    elif \"focal_loss_BIOU\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "      model = XLMRobertaForTokenClassificationWithFocalLoss.from_pretrained(model_id)\n",
    "\n",
    "      # classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "      new_label2id = new_label2id_BIOU\n",
    "      new_id2label = new_id2label_BIOU\n",
    "      label_map = BIOU_label_map\n",
    "\n",
    "    elif \"focal\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "      model = XLMRobertaForTokenClassificationWithFocalLoss.from_pretrained(model_id)\n",
    "\n",
    "      # classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "      new_label2id = new_label2id_BIO\n",
    "      new_id2label = new_id2label_BIO\n",
    "      label_map = BIO_label_map\n",
    "\n",
    "    elif \"crf_BIOU\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", token=access_token)\n",
    "      model = XLMRobertaWithCRF.from_pretrained(model_id)\n",
    "\n",
    "      new_label2id = new_label2id_BIOU_CRF\n",
    "      new_id2label = new_id2label_BIOU_CRF\n",
    "      label_map = BIOU_CRF_label_map\n",
    "\n",
    "    elif \"crf_FGM\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", token=access_token)\n",
    "      model = XLMRobertaWithCRF.from_pretrained(model_id)\n",
    "\n",
    "      new_label2id = new_label2id_BIO_CRF\n",
    "      new_id2label = new_id2label_BIO_CRF\n",
    "      label_map = BIO_CRF_label_map\n",
    "\n",
    "\n",
    "    elif \"crf\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", token=access_token)\n",
    "      model = XLMRobertaWithCRF.from_pretrained(model_id)\n",
    "\n",
    "      new_label2id = new_label2id_BIO_CRF\n",
    "      new_id2label = new_id2label_BIO_CRF\n",
    "      label_map = BIO_CRF_label_map\n",
    "\n",
    "    elif \"weight_class_BIOU\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", token=access_token)\n",
    "      model = XLMRobertaForTokenClassificationWithClassWeight.from_pretrained(model_id, token=access_token, ignore_mismatched_sizes=True)\n",
    "\n",
    "      new_label2id = new_label2id_BIOU\n",
    "      new_id2label = new_id2label_BIOU\n",
    "      label_map = BIOU_label_map\n",
    "\n",
    "    elif \"weight_class\" in model_id:\n",
    "\n",
    "      tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\", token=access_token)\n",
    "      model = XLMRobertaForTokenClassificationWithClassWeight.from_pretrained(model_id, token=access_token, ignore_mismatched_sizes=True)\n",
    "\n",
    "      new_label2id = new_label2id_BIO\n",
    "      new_id2label = new_id2label_BIO\n",
    "      label_map = BIO_label_map\n",
    "\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_id, token=access_token)\n",
    "        # print(next(model.parameters()).device)\n",
    "        classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "        if ( \"BIOU\" in model_id ) or ( \"FGM_BIOU\" in model_id ) :\n",
    "            new_label2id = new_label2id_BIOU\n",
    "            new_id2label = new_id2label_BIOU\n",
    "            label_map = BIOU_label_map\n",
    "        elif \"BIO\" in model_id:\n",
    "            new_label2id = new_label2id_BIO\n",
    "            new_id2label = new_id2label_BIO\n",
    "            label_map = BIO_label_map\n",
    "        elif \"FGM\" in model_id:\n",
    "            new_label2id = new_label2id_BIO\n",
    "            new_id2label = new_id2label_BIO\n",
    "            label_map = BIO_label_map\n",
    "\n",
    "\n",
    "    total_time = 1\n",
    "\n",
    "    if \"level\" in model_id:\n",
    "      total_time = 3\n",
    "\n",
    "    for times in range(total_time):\n",
    "      for line in test_data:\n",
    "          line = line.strip()\n",
    "          name, text = line.split(\"\\t\")\n",
    "\n",
    "          answer_list = []  # 初始化\n",
    "\n",
    "          if choose_loss == 0:\n",
    "\n",
    "                  if \"crf_BIOU\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner_BIOUL(pre)\n",
    "                  elif \"focal_loss_BIOU\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner_BIOUL(pre)\n",
    "                  elif \"weight_class_BIOU\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner_BIOUL(pre)\n",
    "                  elif ( \"BIOU\" in model_id ) or ( \"FGM_BIOU\" in model_id ) :\n",
    "                      pre = classifier(text)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner_BIOUL(pre)\n",
    "                  elif \"BIO\" in model_id:\n",
    "                      pre = classifier(text)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner(pre)\n",
    "                  elif \"focal\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      # pre = classifier(text)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner(pre)\n",
    "                  elif \"crf_FGM\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner(pre)\n",
    "                  elif \"crf\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner(pre)\n",
    "                  elif \"weight_class\" in model_id:\n",
    "                      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner(pre)\n",
    "                  elif \"FGM\" in model_id:\n",
    "                      pre = classifier(text)\n",
    "                      if len(pre) != 0:\n",
    "                        answer_list = Process_Predict_Ner(pre)\n",
    "\n",
    "                  for i in answer_list:\n",
    "                      answer += f\"{name}\\t{i['entity']}\\t{i['start']}\\t{i['end']}\\t{i['word']}\\n\"\n",
    "          else:\n",
    "\n",
    "                if times == 0:\n",
    "                    pre = get_level2_entities_1(model, tokenizer, text, label_map_big, label_map, choose_loss)\n",
    "                elif times == 1:\n",
    "                    pre = get_level2_entities_2(model, tokenizer, text, label_map, choose_loss)\n",
    "                else:\n",
    "                    pre = get_level2_entities_3(model, tokenizer, text, label_map_big, label_map, choose_loss)\n",
    "\n",
    "                if len(pre) != 0:\n",
    "                    answer_list = Process_Predict_level(pre)\n",
    "                    for i in answer_list:\n",
    "                        answer += f\"{name}\\t{i['entity']}\\t{i['start']}\\t{i['end']}\\t{i['word']}\\n\"\n",
    "\n",
    "      name = model_id[model_id.find(\"model\"):]\n",
    "      file_name = f\"./pre_answer_{ name }_{choose_loss}.txt\"\n",
    "\n",
    "      with open( file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "          f.write(answer)\n",
    "\n",
    "      macro_f1 = evaluate_task2( file_name, \"./entity_token_indices.txt\" )\n",
    "\n",
    "\n",
    "      results.append({\n",
    "          \"Model\": model_id,\n",
    "          \"Choose Loss\": choose_loss,\n",
    "          \"F1 Score\": round(macro_f1, 4),\n",
    "          \"total_time\": total_time,\n",
    "          \"now_time\": times\n",
    "      })\n",
    "\n",
    "# 🔸 最後整理成表格\n",
    "print(tabulate(results, headers=\"keys\", tablefmt=\"github\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXgr1NEQQmTm"
   },
   "source": [
    "### **=============================================================================================**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
