{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuE00l4ffoYt",
        "outputId": "88757c1e-6d35-45ed-86c5-99d199f8ebbb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config_argument = json.load(f)\n",
        "\n",
        "access_token = config_argument[\"huggingface_access_token\"]\n",
        "\n",
        "task1_train_data_path_txt = config_argument[\"model_train_task1_data_path_txt\"]\n",
        "task2_train_data_path_txt = config_argument[\"model_train_task2_data_path_txt\"]\n",
        "\n",
        "task1_val_data_path_txt = config_argument[\"model_val_task1_data_path_txt\"]\n",
        "task2_val_data_path_txt = config_argument[\"model_val_task2_data_path_txt\"]\n",
        "\n",
        "answer_val_data_path_txt = config_argument[\"answer_val_data_path_txt\"]\n",
        "\n",
        "model_save_path = config_argument[\"model_save_path\"]\n",
        "model_logging_dir = config_argument[\"model_logging_dir\"]\n",
        "\n",
        "\n",
        "print( \"access_token: \", access_token )\n",
        "print( \"task1_train_data_path: \", task1_train_data_path_txt )\n",
        "print( \"task2_train_data_path: \", task2_train_data_path_txt )\n",
        "print( \"task1_val_data_path: \", task1_val_data_path_txt )\n",
        "print( \"task2_val_data_path: \", task2_val_data_path_txt )\n",
        "print( \"answer_val_data_path: \", answer_val_data_path_txt )\n",
        "print( \"model_save_path: \", model_save_path )\n",
        "print( \"model_logging_dir: \", model_logging_dir )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsUarUIMepPd",
        "outputId": "a84c37c8-1948-4ccd-f02b-d531d9405d19"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
        "from transformers import pipeline\n",
        "\n",
        "access_token = access_token\n",
        "\n",
        "model_id = \"xlm-roberta-large-finetuned-conll03-english\"  # xlm-roberta-large-finetuned-conll03-english\n",
        "\n",
        "from transformers import pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token ) #padding=True,truncation=True,max_length=128\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_id, token=access_token)\n",
        "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcbpQt6u67JO",
        "outputId": "f21b0acf-25f8-4657-b56c-f2582adccb3b"
      },
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹åŸå§‹ dropout è¨­å®š\n",
        "print(\"hidden_dropout_prob:\", model.roberta.config.hidden_dropout_prob)\n",
        "print(\"attention_probs_dropout_prob:\", model.roberta.config.attention_probs_dropout_prob)\n",
        "\n",
        "# æŸ¥çœ‹å¯¦éš› Dropout å±¤çš„æ©Ÿç‡\n",
        "print(\"model.dropout.p:\", model.dropout.p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIjhUSQcwu0_",
        "outputId": "19403d42-54d7-46a4-eca6-e54fb5221d5b"
      },
      "outputs": [],
      "source": [
        "print(\"Special tokens:\")\n",
        "for name in tokenizer.special_tokens_map:\n",
        "    token = tokenizer.special_tokens_map[name]\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"{name:20} -> {token:10} (ID: {token_id})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22_lOQgmhSZV",
        "outputId": "96723f77-d0c0-4717-93ce-cd35cdf412be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', 'â–really', 'â–I', 'â–used', 'â–to', 'â–have', 'â–that', '.', 'â–I', 'â–used', 'â–to', 'â–get', 'â–excited', 'â–when', 'â–I', 'â–could', 'â–have', 'â–the', 'â–afternoon', 'â–to', 'â–myself', ',', 'â–but', 'â–I', 'â–don', \"'\", 't', 'â–really', 'â–have', 'â–that', 'â–any', 'â–more', '.', 'â–I', 'â–try', 'â–to', 'â–fill', 'â–my', 'â–time', 'â–with', 'â–other', 'â–things', '.', 'â–On', 'â–Wednesday', 's', ',', 'â–for', 'â–example', ',', 'â–Am', 'elia', 'â–is', 'â–at', 'â–school', 'â–until', 'â–ni', 'ne', 'â–o', \"'\", 'c', 'lock', 'â–at', 'â–night', ',', 'â–so', 'â–Wednesday', 'â–is', 'â–like', 'â–my', 'â–night', 'â–to', 'â–try', 'â–to', 'â–go', 'â–see', 'â–some', 'â–friends', 'â–or', 'â–something', 'â–like', 'â–that', ',', 'â–and', 'â–keep', 'â–myself', 'â–stay', 'â–at', 'â–work', 'â–late', 'â–or', 'â–do', 'â–something', 'â–right', 'â–after', 'â–work', ',', 'â–so', 'â–I', \"'\", 'm', 'â–not', 'â–just', 'â–like', 'â–at', 'â–home', 'â–by', 'â–myself', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer(\"really I used to have that. I used to get excited when I could have the afternoon to myself, but I don't really have that any more. I try to fill my time with other things. On Wednesdays, for example, Amelia is at school until nine o'clock at night, so Wednesday is like my night to try to go see some friends or something like that, and keep myself stay at work late or do something right after work, so I'm not just like at home by myself.\", return_offsets_mapping=True)\n",
        "print(tokens.tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkD8Ie7gh_3i",
        "outputId": "929150bd-03f9-41e3-8422-b7df25db96b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'I-LOC', 4: 'I-MISC', 5: 'I-ORG', 6: 'I-PER', 7: 'O'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "config = AutoConfig.from_pretrained(model_id, token=access_token)\n",
        "print(config.id2label)  # é€™æœƒåˆ—å‡º id å°æ‡‰çš„ label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5TtRUjCGjYMm"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    0: 'O',\n",
        "    1: 'B-PATIENT', 2: 'I-PATIENT', 3: 'L-PATIENT', 4: 'U-PATIENT',\n",
        "    5: 'B-DOCTOR', 6: 'I-DOCTOR', 7: 'L-DOCTOR', 8: 'U-DOCTOR',\n",
        "    9: 'B-USERNAME', 10: 'I-USERNAME', 11: 'L-USERNAME', 12: 'U-USERNAME',\n",
        "    13: 'B-FAMILYNAME', 14: 'I-FAMILYNAME', 15: 'L-FAMILYNAME', 16: 'U-FAMILYNAME',\n",
        "    17: 'B-PERSONALNAME', 18: 'I-PERSONALNAME', 19: 'L-PERSONALNAME', 20: 'U-PERSONALNAME',\n",
        "    21: 'B-PROFESSION', 22: 'I-PROFESSION', 23: 'L-PROFESSION', 24: 'U-PROFESSION',\n",
        "    25: 'B-ROOM', 26: 'I-ROOM', 27: 'L-ROOM', 28: 'U-ROOM',\n",
        "    29: 'B-DEPARTMENT', 30: 'I-DEPARTMENT', 31: 'L-DEPARTMENT', 32: 'U-DEPARTMENT',\n",
        "    33: 'B-HOSPITAL', 34: 'I-HOSPITAL', 35: 'L-HOSPITAL', 36: 'U-HOSPITAL',\n",
        "    37: 'B-ORGANIZATION', 38: 'I-ORGANIZATION', 39: 'L-ORGANIZATION', 40: 'U-ORGANIZATION',\n",
        "    41: 'B-STREET', 42: 'I-STREET', 43: 'L-STREET', 44: 'U-STREET',\n",
        "    45: 'B-CITY', 46: 'I-CITY', 47: 'L-CITY', 48: 'U-CITY',\n",
        "    49: 'B-DISTRICT', 50: 'I-DISTRICT', 51: 'L-DISTRICT', 52: 'U-DISTRICT',\n",
        "    53: 'B-COUNTY', 54: 'I-COUNTY', 55: 'L-COUNTY', 56: 'U-COUNTY',\n",
        "    57: 'B-STATE', 58: 'I-STATE', 59: 'L-STATE', 60: 'U-STATE',\n",
        "    61: 'B-COUNTRY', 62: 'I-COUNTRY', 63: 'L-COUNTRY', 64: 'U-COUNTRY',\n",
        "    65: 'B-ZIP', 66: 'I-ZIP', 67: 'L-ZIP', 68: 'U-ZIP',\n",
        "    69: 'B-LOCATION-OTHER', 70: 'I-LOCATION-OTHER', 71: 'L-LOCATION-OTHER', 72: 'U-LOCATION-OTHER',\n",
        "    73: 'B-AGE', 74: 'I-AGE', 75: 'L-AGE', 76: 'U-AGE',\n",
        "    77: 'B-DATE', 78: 'I-DATE', 79: 'L-DATE', 80: 'U-DATE',\n",
        "    81: 'B-TIME', 82: 'I-TIME', 83: 'L-TIME', 84: 'U-TIME',\n",
        "    85: 'B-DURATION', 86: 'I-DURATION', 87: 'L-DURATION', 88: 'U-DURATION',\n",
        "    89: 'B-SET', 90: 'I-SET', 91: 'L-SET', 92: 'U-SET',\n",
        "    93: 'B-PHONE', 94: 'I-PHONE', 95: 'L-PHONE', 96: 'U-PHONE',\n",
        "    97: 'B-FAX', 98: 'I-FAX', 99: 'L-FAX', 100: 'U-FAX',\n",
        "    101: 'B-EMAIL', 102: 'I-EMAIL', 103: 'L-EMAIL', 104: 'U-EMAIL',\n",
        "    105: 'B-URL', 106: 'I-URL', 107: 'L-URL', 108: 'U-URL',\n",
        "    109: 'B-IPADDRESS', 110: 'I-IPADDRESS', 111: 'L-IPADDRESS', 112: 'U-IPADDRESS',\n",
        "    113: 'B-SOCIAL_SECURITY_NUMBER', 114: 'I-SOCIAL_SECURITY_NUMBER', 115: 'L-SOCIAL_SECURITY_NUMBER', 116: 'U-SOCIAL_SECURITY_NUMBER',\n",
        "    117: 'B-MEDICAL_RECORD_NUMBER', 118: 'I-MEDICAL_RECORD_NUMBER', 119: 'L-MEDICAL_RECORD_NUMBER', 120: 'U-MEDICAL_RECORD_NUMBER',\n",
        "    121: 'B-HEALTH_PLAN_NUMBER', 122: 'I-HEALTH_PLAN_NUMBER', 123: 'L-HEALTH_PLAN_NUMBER', 124: 'U-HEALTH_PLAN_NUMBER',\n",
        "    125: 'B-ACCOUNT_NUMBER', 126: 'I-ACCOUNT_NUMBER', 127: 'L-ACCOUNT_NUMBER', 128: 'U-ACCOUNT_NUMBER',\n",
        "    129: 'B-LICENSE_NUMBER', 130: 'I-LICENSE_NUMBER', 131: 'L-LICENSE_NUMBER', 132: 'U-LICENSE_NUMBER',\n",
        "    133: 'B-VEHICLE_ID', 134: 'I-VEHICLE_ID', 135: 'L-VEHICLE_ID', 136: 'U-VEHICLE_ID',\n",
        "    137: 'B-DEVICE_ID', 138: 'I-DEVICE_ID', 139: 'L-DEVICE_ID', 140: 'U-DEVICE_ID',\n",
        "    141: 'B-BIOMETRIC_ID', 142: 'I-BIOMETRIC_ID', 143: 'L-BIOMETRIC_ID', 144: 'U-BIOMETRIC_ID',\n",
        "    145: 'B-ID_NUMBER', 146: 'I-ID_NUMBER', 147: 'L-ID_NUMBER', 148: 'U-ID_NUMBER',\n",
        "    149: 'B-OTHER', 150: 'I-OTHER', 151: 'L-OTHER', 152: 'U-OTHER'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pJ4iAcwmSQO"
      },
      "source": [
        "è³‡æ–™æº–å‚™\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2SdsW_ccnnwr"
      },
      "outputs": [],
      "source": [
        "def Caculate_Wav_File_Times( inputs ) :\n",
        "\n",
        "        read = inputs\n",
        "\n",
        "        dict_times = {}\n",
        "        for line in read:\n",
        "            line = line.strip()\n",
        "            line_split = line.split('\\t')\n",
        "\n",
        "            if line_split[0] not in dict_times :\n",
        "                dict_times[line_split[0]] = 1\n",
        "            else:\n",
        "                dict_times[line_split[0]] = dict_times[line_split[0]]  + 1\n",
        "\n",
        "        return dict_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sFDTcWSqmRxS"
      },
      "outputs": [],
      "source": [
        "# with open( \"/content/task2_answer.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
        "#   data = f.readlines()\n",
        "\n",
        "def Prepare_Task2_NER(data) :\n",
        "  data_times_dict = Caculate_Wav_File_Times( data )\n",
        "\n",
        "\n",
        "  data_list = {}\n",
        "  temp_dict = {}\n",
        "  temp_list = []\n",
        "\n",
        "  while data :\n",
        "\n",
        "    times = data_times_dict[data[0].split('\\t')[0]]\n",
        "\n",
        "    for i in range( times  ) :\n",
        "\n",
        "      line = data[i]\n",
        "\n",
        "\n",
        "      line = line.strip()\n",
        "      line_split = line.split(\"\\t\")\n",
        "\n",
        "\n",
        "\n",
        "      temp_dict[ line_split[4] ] = line_split[1]\n",
        "      temp_list.append( temp_dict )\n",
        "      temp_dict = {}\n",
        "\n",
        "    data_list[ data[0].split('\\t')[0] ] = temp_list\n",
        "    temp_list = []\n",
        "\n",
        "\n",
        "    data = data[times:]\n",
        "\n",
        "\n",
        "  # print(data_list)\n",
        "\n",
        "  return data_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaQEOcNxpIgS",
        "outputId": "71084300-a44a-40d0-b7ef-f0437c91485f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "940\n"
          ]
        }
      ],
      "source": [
        "#en\n",
        "with open( task2_train_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
        "  data = f.readlines()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_2/task2_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
        "#   data = data + f.readlines()\n",
        "\n",
        "\n",
        "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_3/task2_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
        "#   data = data + f.readlines()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_list = Prepare_Task2_NER( data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd5oRWmQ4zp0",
        "outputId": "5bad583b-49f5-4e01-9010-5aa33f1dbfa9"
      },
      "outputs": [],
      "source": [
        "for i in data_list :\n",
        "  print(i, data_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHUSt8h84bXs",
        "outputId": "a2adc70c-5dc6-4f68-8b4e-2eb981ac7fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1275\n"
          ]
        }
      ],
      "source": [
        "#en\n",
        "with open( task2_val_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
        "  val_data = f.readlines()\n",
        "\n",
        "print(len(val_data))\n",
        "\n",
        "val_data_list = Prepare_Task2_NER( val_data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6L2AhJq5N0O",
        "outputId": "d52d069b-58cb-454f-c4a5-f3521cd8cb75"
      },
      "outputs": [],
      "source": [
        "for i in val_data_list :\n",
        "  print(i, val_data_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpzseulNlSjL",
        "outputId": "5ff6c609-bf1d-4f4b-d10d-9cd43ad9accb"
      },
      "outputs": [],
      "source": [
        "new_label2id= {v: k for k, v in label_map.items()}\n",
        "new_id2label = label_map\n",
        "\n",
        "# æ›´æ–°é…ç½®\n",
        "config.id2label = new_id2label\n",
        "config.label2id = new_label2id\n",
        "\n",
        "# æ‰“å°æ–°çš„ id2label å’Œ label2id\n",
        "print(\"æ–°çš„ id2label:\", config.id2label)\n",
        "print(\"æ–°çš„ label2id:\", config.label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "othNXXXQs0Gp"
      },
      "outputs": [],
      "source": [
        "def Prepare_Task1_NER( data, data_list):\n",
        "  train_data = []\n",
        "\n",
        "  for i in data:\n",
        "      # print( i )\n",
        "      line = i.strip()\n",
        "      line_split = line.split(\"\\t\")\n",
        "\n",
        "      name = line_split[0]\n",
        "      text = line_split[1]\n",
        "\n",
        "      tokens = tokenizer(text.strip(), return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
        "      offsets = tokens[\"offset_mapping\"][0].tolist()\n",
        "      input_ids = tokens[\"input_ids\"][0].tolist()\n",
        "      token_texts = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "      # åˆå§‹åŒ– label\n",
        "      label = [config.label2id[\"O\"]] * len(input_ids)\n",
        "      label[0] = -100\n",
        "      label[-1] = -100\n",
        "\n",
        "      input_ids = tokens[\"input_ids\"]\n",
        "      attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "\n",
        "      # print(f\"name: {name}\")\n",
        "      # print(f\"text: {text}\")\n",
        "      # print(f\"offsets: {offsets}\")\n",
        "      # print(tokens.tokens())\n",
        "\n",
        "\n",
        "      # æª¢æŸ¥æ˜¯å¦æœ‰æ¨™è¨»è³‡æ–™\n",
        "      if name not in data_list:\n",
        "          train_data.append({\n",
        "              \"input_ids\": input_ids[0].tolist(),\n",
        "              \"labels\": label,\n",
        "              \"attention_mask\": attention_mask[0].tolist()\n",
        "          })\n",
        "          continue\n",
        "\n",
        "      # å°‡æ¨™è¨»åˆä½µç‚º (start, end, tag) çš„æ ¼å¼\n",
        "      entities = []\n",
        "      used_indices = set()  # é˜²æ­¢é‡è¤‡ä½¿ç”¨ç›¸åŒæ–‡å­—\n",
        "      for ent in data_list[name]:\n",
        "          for word, tag in ent.items():\n",
        "              # ç”¨ sliding window å°‹æ‰¾æ²’ä½¿ç”¨éçš„ word ä½ç½®\n",
        "              start = -1\n",
        "              for idx in range(len(text)):\n",
        "                  if idx in used_indices:\n",
        "                      continue\n",
        "                  if text[idx:idx+len(word)] == word:\n",
        "                      start = idx\n",
        "                      # æ¨™è¨˜é€™äº›å­—å…ƒä½ç½®å·²ç¶“ç”¨é\n",
        "                      used_indices.update(range(start, start+len(word)))\n",
        "                      # print(used_indices)\n",
        "                      break\n",
        "              if start != -1:\n",
        "                  end = start + len(word)\n",
        "                  entities.append((start, end, tag))\n",
        "              else:\n",
        "                  print(f\"[æœªæ‰¾åˆ°å¯¦é«”] name={name}, word='{word}', tag='{tag}'\")\n",
        "                  print(f\"â†’ åŸå§‹å¥å­ï¼š{text}\")\n",
        "                  print(text[idx:idx+len(word)])\n",
        "\n",
        "      # print(f\"name: {name}\")\n",
        "      # print(f\"text: {text}\")\n",
        "      # print(f\"entities: {entities}\")\n",
        "      # print( offsets )\n",
        "\n",
        "      # æ¯”å° offset å’Œ entity spanï¼Œæ¨™è¨» label\n",
        "      for idx, (start, end) in enumerate(offsets):\n",
        "\n",
        "        if start == end:\n",
        "            continue\n",
        "        for ent_start, ent_end, tag in entities:\n",
        "            if start >= ent_start and end <= ent_end:\n",
        "                if ent_start == start and ent_end == end:  # å–® token å¯¦é«”\n",
        "                    label[idx] = config.label2id[f\"U-{tag}\"]\n",
        "                    break\n",
        "                elif start == ent_start:\n",
        "                    label[idx] = config.label2id[f\"B-{tag}\"]\n",
        "                    break\n",
        "                elif end == ent_end:\n",
        "                    label[idx] = config.label2id[f\"L-{tag}\"]\n",
        "                    break\n",
        "                else:\n",
        "                    label[idx] = config.label2id[f\"I-{tag}\"]\n",
        "                    break\n",
        "\n",
        "      train_data.append({\n",
        "          \"input_ids\": input_ids[0].tolist(),\n",
        "          \"labels\": label,\n",
        "          \"attention_mask\": attention_mask[0].tolist()\n",
        "      })\n",
        "\n",
        "  return train_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBMotjWZ8tLh",
        "outputId": "248b819d-c9ee-4768-feaa-fca08a5c42b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "310\n"
          ]
        }
      ],
      "source": [
        "#en\n",
        "with open( task1_train_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
        "  data = f.readlines()\n",
        "\n",
        "print( len(data) )\n",
        "\n",
        "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_2/task1_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
        "#   data = data + f.readlines()\n",
        "\n",
        "# with open( \"/content/drive/MyDrive/AICUP_DATA/en-dataset/3_fold/hold_3/task1_answer_change.txt\", \"r\", encoding=\"utf-8\" ) as f :\n",
        "#   data = data + f.readlines()\n",
        "\n",
        "\n",
        "train_data = Prepare_Task1_NER( data, data_list )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu5_uFrb91D_",
        "outputId": "6d01d5f8-633f-4798-d072-23ca6e79aee9"
      },
      "outputs": [],
      "source": [
        "for i in train_data[-5:-1]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOMDcgpR5_Ap",
        "outputId": "8322f34e-2fc9-4b56-e6a1-e4ae5fc7607e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "449\n"
          ]
        }
      ],
      "source": [
        "#en\n",
        "with open( task1_val_data_path_txt, \"r\", encoding=\"utf-8\" ) as f :\n",
        "  val_data = f.readlines()\n",
        "\n",
        "print( len(val_data) )\n",
        "\n",
        "\n",
        "\n",
        "test_data = Prepare_Task1_NER( val_data, val_data_list )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7R4zkKLPLpy",
        "outputId": "1cfc2f90-7f0b-4be8-d1ed-48317c14c7d3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eXXHK3Ts9uXB"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "data_train = Dataset.from_list(train_data)\n",
        "data_test = Dataset.from_list(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bXWWzQuPRFC",
        "outputId": "5842da1a-7d37-442f-b940-beec07e5a4a7"
      },
      "outputs": [],
      "source": [
        "data_train, data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw2qsS01QCwo",
        "outputId": "988bc4f5-6f97-45e8-d5f2-eb64d770f383"
      },
      "outputs": [],
      "source": [
        "# model.config.label2id = new_label2id\n",
        "# model.config.id2label = new_id2label\n",
        "\n",
        "model_id = \"xlm-roberta-large-finetuned-conll03-english\"   #xlm-roberta-large-finetuned-conll03-english\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
        "\n",
        "# âœ… ä½¿ç”¨ data collator è™•ç† paddingï¼ˆé€™æœƒè‡ªå‹• padding input_ids, attention_mask, å’Œ labelsï¼‰\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=len(new_label2id),\n",
        "    id2label=new_id2label,\n",
        "    label2id=new_label2id,\n",
        "    ignore_mismatched_sizes=True   # é€™è¡Œå¾ˆé‡è¦ï¼\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nVclN36lGaIV"
      },
      "outputs": [],
      "source": [
        "def bio_u_to_bio_es(labels):\n",
        "    new_labels = []\n",
        "    for label in labels:\n",
        "        if label.startswith('L-'):\n",
        "            new_labels.append('E-' + label[2:])\n",
        "        elif label.startswith('U-'):\n",
        "            new_labels.append('S-' + label[2:])\n",
        "        else:\n",
        "            new_labels.append(label)\n",
        "    return new_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "fn5OeA6ChQ9v"
      },
      "outputs": [],
      "source": [
        "def Process_Predict_Ner_BIOUL(pre):\n",
        "    answer_list = []\n",
        "    current_entity = None\n",
        "    current_word = \"\"\n",
        "    start_pos = None\n",
        "    end_pos = None\n",
        "\n",
        "    for dic in pre:\n",
        "        entity_type = dic['entity']\n",
        "        raw_word = dic['word']\n",
        "        word = raw_word.replace(\"â–\", \"\")\n",
        "        token_start = dic.get('start')\n",
        "        token_end = dic.get('end')\n",
        "        has_space = raw_word.startswith(\"â–\")\n",
        "\n",
        "        if entity_type.startswith(\"B-\"):\n",
        "            if current_entity and current_word:\n",
        "                answer_list.append({\n",
        "                    \"entity\": current_entity,\n",
        "                    \"word\": current_word,\n",
        "                    \"start\": start_pos,\n",
        "                    \"end\": end_pos\n",
        "                })\n",
        "            current_entity = entity_type.replace(\"B-\", \"\")\n",
        "            current_word = word\n",
        "            start_pos = token_start\n",
        "            end_pos = token_end\n",
        "\n",
        "        elif entity_type.startswith(\"I-\"):\n",
        "            ent = entity_type.replace(\"I-\", \"\")\n",
        "            if current_entity == ent:\n",
        "                if has_space:\n",
        "                    current_word += \" \" + word\n",
        "                else:\n",
        "                    current_word += word\n",
        "                end_pos = token_end\n",
        "            else:\n",
        "                if current_entity and current_word:\n",
        "                    answer_list.append({\n",
        "                        \"entity\": current_entity,\n",
        "                        \"word\": current_word,\n",
        "                        \"start\": start_pos,\n",
        "                        \"end\": end_pos\n",
        "                    })\n",
        "                current_entity = ent\n",
        "                current_word = word\n",
        "                start_pos = token_start\n",
        "                end_pos = token_end\n",
        "\n",
        "        elif entity_type.startswith(\"L-\"):\n",
        "            ent = entity_type.replace(\"L-\", \"\")\n",
        "            if current_entity == ent:\n",
        "                if has_space:\n",
        "                    current_word += \" \" + word\n",
        "                else:\n",
        "                    current_word += word\n",
        "                end_pos = token_end\n",
        "                answer_list.append({\n",
        "                    \"entity\": current_entity,\n",
        "                    \"word\": current_word,\n",
        "                    \"start\": start_pos,\n",
        "                    \"end\": end_pos\n",
        "                })\n",
        "                current_entity = None\n",
        "                current_word = \"\"\n",
        "                start_pos = None\n",
        "                end_pos = None\n",
        "            else:\n",
        "                # å¦‚æœä¹‹å‰çš„ entity æ²’æ¥ä¸Šï¼Œç•¶ä½œç¨ç«‹å¯¦é«”è™•ç†\n",
        "                answer_list.append({\n",
        "                    \"entity\": ent,\n",
        "                    \"word\": word,\n",
        "                    \"start\": token_start,\n",
        "                    \"end\": token_end\n",
        "                })\n",
        "                current_entity = None\n",
        "                current_word = \"\"\n",
        "                start_pos = None\n",
        "                end_pos = None\n",
        "\n",
        "        elif entity_type.startswith(\"U-\"):\n",
        "            ent = entity_type.replace(\"U-\", \"\")\n",
        "            answer_list.append({\n",
        "                \"entity\": ent,\n",
        "                \"word\": word,\n",
        "                \"start\": token_start,\n",
        "                \"end\": token_end\n",
        "            })\n",
        "\n",
        "            current_entity = None\n",
        "            current_word = \"\"\n",
        "            start_pos = None\n",
        "            end_pos = None\n",
        "\n",
        "        else:  # O\n",
        "            if current_entity and current_word:\n",
        "                answer_list.append({\n",
        "                    \"entity\": current_entity,\n",
        "                    \"word\": current_word,\n",
        "                    \"start\": start_pos,\n",
        "                    \"end\": end_pos\n",
        "                })\n",
        "            current_entity = None\n",
        "            current_word = \"\"\n",
        "            start_pos = None\n",
        "            end_pos = None\n",
        "\n",
        "    # æ”¶å°¾\n",
        "    if current_entity and current_word:\n",
        "        answer_list.append({\n",
        "            \"entity\": current_entity,\n",
        "            \"word\": current_word,\n",
        "            \"start\": start_pos,\n",
        "            \"end\": end_pos\n",
        "        })\n",
        "\n",
        "    return answer_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ua81z_01hRGV"
      },
      "outputs": [],
      "source": [
        "def get_level2_entities_normal(model, tokenizer, sentence, label_map):\n",
        "    device = next(model.parameters()).device  # å–å¾— model è£ç½®\n",
        "\n",
        "    # 1. Tokenize with offsets\n",
        "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)          # æ”¾åˆ° GPU\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)  # æ”¾åˆ° GPU\n",
        "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu())  # token ids æ”¾ CPU æ‰èƒ½ç”¨ tokenizer\n",
        "\n",
        "    # 2. Model forward\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # [batch_size, seq_len, num_labels]\n",
        "\n",
        "    preds = torch.argmax(logits, dim=2)[0].cpu().numpy()  # é æ¸¬çµæœæ”¾å› CPU\n",
        "\n",
        "    results = []\n",
        "    for idx, (pred_id, offset) in enumerate(zip(preds, offsets)):\n",
        "        token_id = input_ids[0, idx].item()\n",
        "\n",
        "        # è·³éç‰¹æ®Š token æˆ–ç„¡æ•ˆ offset\n",
        "        if token_id in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]:\n",
        "            continue\n",
        "\n",
        "        start, end = offset\n",
        "        entity = label_map.get(pred_id, \"O\")\n",
        "\n",
        "        if entity != \"O\":\n",
        "            probs = torch.softmax(logits[0, idx], dim=0)\n",
        "            score = probs[pred_id].item()\n",
        "\n",
        "            results.append({\n",
        "                \"entity\": entity,\n",
        "                \"score\": np.float32(score),\n",
        "                \"index\": idx,\n",
        "                \"word\": tokens[idx],  # æ›´æº–ç¢º\n",
        "                \"start\": start,\n",
        "                \"end\": end\n",
        "            })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kvhf3fYGhQBo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_overlap(pred_start, pred_end, gt_start, gt_end):\n",
        "    \"\"\"è¨ˆç®—å…©å€‹æ™‚é–“å€é–“çš„é‡ç–Šé•·åº¦\"\"\"\n",
        "    overlap_start = max(pred_start, gt_start)\n",
        "    overlap_end = min(pred_end, gt_end)\n",
        "    overlap = max(0, overlap_end - overlap_start)\n",
        "    return overlap\n",
        "\n",
        "def evaluate_task2( ground_truth_file, model, tokenizer ) :\n",
        "\n",
        "\n",
        "\n",
        "    answer = \"\"\n",
        "\n",
        "    for text in val_data :\n",
        "\n",
        "      answer_list = []\n",
        "\n",
        "      text_split = text.strip().split(\"\\t\")\n",
        "      name = text_split[0]\n",
        "      text = text_split[1]\n",
        "\n",
        "      pre = get_level2_entities_normal(model, tokenizer, text, label_map)\n",
        "      if len(pre) != 0:\n",
        "        answer_list = Process_Predict_Ner_BIOUL(pre)\n",
        "\n",
        "      for i in answer_list:\n",
        "        answer += f\"{name}\\t{i['entity']}\\t{i['start']}\\t{i['end']}\\t{i['word']}\\n\"\n",
        "\n",
        "    prediction_file = \"model_eval.txt\"\n",
        "    with open( prediction_file, \"w\", encoding=\"utf-8\") as f:\n",
        "      f.write(answer)\n",
        "\n",
        "\n",
        "    # è®€å–é æ¸¬å’ŒçœŸå¯¦æ¨™ç±¤æ•¸æ“š\n",
        "    import csv\n",
        "    pred_df = pd.read_csv(\n",
        "          prediction_file,\n",
        "          sep='\\t',\n",
        "          header=None,\n",
        "          names=['id', 'type', 'start', 'end', 'content'],\n",
        "          quoting=csv.QUOTE_NONE,        # ä¸è§£æå¼•è™Ÿ\n",
        "          encoding='utf-8',              # æˆ–è©¦ utf-8-sig\n",
        "          on_bad_lines='skip',           # è·³éçˆ›è¡Œ\n",
        "          engine='python'                # æ›´å¯¬å®¹çš„ parser\n",
        "      )\n",
        "    gt_df = pd.read_csv(ground_truth_file, sep='\\t', header=None,\n",
        "                       names=['id', 'type', 'start', 'end', 'content'])\n",
        "\n",
        "    # ç²å–æ‰€æœ‰ç¨ç‰¹çš„SHIé¡å‹\n",
        "    all_types = sorted(set(gt_df['type'].unique()) | set(pred_df['type'].unique()))\n",
        "\n",
        "    # åˆå§‹åŒ–æ¯ç¨®é¡å‹çš„æŒ‡æ¨™\n",
        "    metrics = {shi_type: {'tp': 0, 'fp': 0, 'fn': 0} for shi_type in all_types}\n",
        "\n",
        "    # æŒ‰éŸ³é »IDåˆ†çµ„è™•ç†\n",
        "    unique_ids = sorted(set(gt_df['id'].unique()) | set(pred_df['id'].unique()))\n",
        "\n",
        "    for audio_id in unique_ids:\n",
        "        gt_records = gt_df[gt_df['id'] == audio_id].copy()\n",
        "        pred_records = pred_df[pred_df['id'] == audio_id].copy()\n",
        "\n",
        "        # åˆå§‹åŒ–åŒ¹é…çŸ©é™£ä¾†è¿½è¹¤å·²è™•ç†çš„é æ¸¬å’ŒçœŸå¯¦æ¨™ç±¤\n",
        "        gt_matched = [False] * len(gt_records)\n",
        "        pred_matched = [False] * len(pred_records)\n",
        "\n",
        "        # è¨ˆç®—True Positiveså’Œéƒ¨åˆ†False Positives/False Negatives\n",
        "        for i, pred_row in enumerate(pred_records.itertuples()):\n",
        "            pred_type = pred_row.type\n",
        "            pred_start = pred_row.start\n",
        "            pred_end = pred_row.end\n",
        "            pred_duration = pred_end - pred_start\n",
        "\n",
        "            best_overlap = 0\n",
        "            best_gt_idx = -1\n",
        "\n",
        "            # æ‰¾åˆ°èˆ‡ç•¶å‰é æ¸¬é‡ç–Šæœ€å¤§çš„çœŸå¯¦æ¨™ç±¤\n",
        "            for j, gt_row in enumerate(gt_records.itertuples()):\n",
        "                if gt_row.type != pred_type:\n",
        "                    continue\n",
        "\n",
        "                overlap = calculate_overlap(pred_start, pred_end, gt_row.start, gt_row.end)\n",
        "                if overlap > best_overlap:\n",
        "                    best_overlap = overlap\n",
        "                    best_gt_idx = j\n",
        "\n",
        "            if best_gt_idx >= 0:  # æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…\n",
        "                gt_row = gt_records.iloc[best_gt_idx]\n",
        "                gt_duration = gt_row.end - gt_row.start\n",
        "\n",
        "                # è¨ˆç®— True Positive\n",
        "                metrics[pred_type]['tp'] += best_overlap\n",
        "\n",
        "                # è¨ˆç®— False Positive (å°æ–¼éƒ¨åˆ†åŒ¹é…ï¼Œé¡å‹ç›¸åŒ)\n",
        "                metrics[pred_type]['fp'] += pred_duration - best_overlap\n",
        "\n",
        "                # è¨ˆç®— False Negative (å°æ–¼éƒ¨åˆ†åŒ¹é…ï¼Œé¡å‹ç›¸åŒ)\n",
        "                metrics[pred_type]['fn'] += gt_duration - best_overlap\n",
        "\n",
        "                # æ¨™è¨˜å·²è™•ç†\n",
        "                gt_matched[best_gt_idx] = True\n",
        "                pred_matched[i] = True\n",
        "            else:\n",
        "                # å®Œå…¨ä¸åŒ¹é…æˆ–è€…é¡å‹ä¸åŒï¼šæ•´å€‹é æ¸¬ç‚ºFalse Positive\n",
        "                metrics[pred_type]['fp'] += pred_duration\n",
        "\n",
        "        # è™•ç†æœªåŒ¹é…çš„çœŸå¯¦æ¨™ç±¤ (False Negatives)\n",
        "        for j, matched in enumerate(gt_matched):\n",
        "            if not matched:\n",
        "                gt_row = gt_records.iloc[j]\n",
        "                gt_type = gt_row.type\n",
        "                gt_duration = gt_row.end - gt_row.start\n",
        "                metrics[gt_type]['fn'] += gt_duration\n",
        "\n",
        "        # è™•ç†èˆ‡é¡å‹ä¸åŒçš„é æ¸¬ (False Positives)\n",
        "        for i, (matched, pred_row) in enumerate(zip(pred_matched, pred_records.itertuples())):\n",
        "            if matched:\n",
        "                continue\n",
        "\n",
        "            # æª¢æŸ¥æ˜¯å¦æœ‰èˆ‡å…¶ä»–é¡å‹åŒ¹é…\n",
        "            pred_type = pred_row.type\n",
        "            pred_start = pred_row.start\n",
        "            pred_end = pred_row.end\n",
        "            pred_duration = pred_end - pred_start\n",
        "\n",
        "            for gt_row in gt_records.itertuples():\n",
        "                if gt_row.type == pred_type:\n",
        "                    continue  # å·²åœ¨ä¹‹å‰çš„æ­¥é©Ÿä¸­è™•ç†é\n",
        "\n",
        "                overlap = calculate_overlap(pred_start, pred_end, gt_row.start, gt_row.end)\n",
        "                if overlap > 0:\n",
        "                    # é¡å‹ä¸åŒ¹é…ä½†æ™‚é–“é‡ç–Šï¼šæ•´å€‹é æ¸¬ç‚ºFalse Positive\n",
        "                    metrics[pred_type]['fp'] += pred_duration\n",
        "                    break\n",
        "\n",
        "    # è¨ˆç®—æ¯ç¨®é¡å‹çš„Precision, Recallå’ŒF1\n",
        "    f1_scores = []\n",
        "    for shi_type in all_types:\n",
        "        m = metrics[shi_type]\n",
        "        precision = m['tp'] / (m['tp'] + m['fp']) if (m['tp'] + m['fp']) > 0 else 0\n",
        "        recall = m['tp'] / (m['tp'] + m['fn']) if (m['tp'] + m['fn']) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        # print(f\"é¡å‹ {shi_type}:\")\n",
        "        # print(f\"  Precision: {precision:.4f}\")\n",
        "        # print(f\"  Recall: {recall:.4f}\")\n",
        "        # print(f\"  F1: {f1:.4f}\")\n",
        "        # print(f\"  TP: {m['tp']:.2f}, FP: {m['fp']:.2f}, FN: {m['fn']:.2f}\")\n",
        "        # print()\n",
        "\n",
        "    # è¨ˆç®—å®å¹³å‡F1\n",
        "    macro_f1 = np.mean(f1_scores)\n",
        "    # print(f\"Macro-Average F1: {macro_f1:.4f}\")\n",
        "\n",
        "    return macro_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mmwd7Mf5he5t"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class CharBasedEvaluationCallback(TrainerCallback):\n",
        "    def __init__(self, task2_path, tokenizer):\n",
        "        self.task2_path = task2_path\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        model = kwargs[\"model\"]\n",
        "\n",
        "        macro_f1 = evaluate_task2(self.task2_path, model, self.tokenizer)\n",
        "\n",
        "        print(f\"[Char-based Evaluation after epoch {state.epoch}]\")\n",
        "        print(\"Macro-F1:\", macro_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lfv9J4AW2D6W",
        "outputId": "c248f341-d97e-48ff-e859-4477854f1157"
      },
      "outputs": [],
      "source": [
        "# âœ… å»ºç«‹ Trainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./ner_results\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     logging_strategy=\"epoch\",          # ğŸ‘ˆ æ–°å¢é€™è¡Œï¼šæ¯å€‹ epoch log ä¸€æ¬¡\n",
        "#     logging_first_step=True,           # ğŸ‘ˆ ç¬¬ä¸€æ­¥å°± logï¼ˆå¯é¸ï¼‰\n",
        "#     logging_dir=\"./ner_logs\",          # ğŸ‘ˆ log æª”å„²å­˜è³‡æ–™å¤¾ï¼ˆå¯é¸ï¼‰\n",
        "#     learning_rate=1e-5,\n",
        "#     do_train=True,\n",
        "#     do_eval=True,\n",
        "#     num_train_epochs=50,\n",
        "#     weight_decay=0.01,\n",
        "# )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_save_path,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    logging_first_step=True,\n",
        "    logging_dir=model_logging_dir,\n",
        "    learning_rate=3e-5,                     # å¾®å¹…èª¿é«˜ï¼ˆè¦–æƒ…æ³ï¼‰3e-5\n",
        "    num_train_epochs=20,                   # é¿å…ä¸€æ¬¡å°±è¨­ 50\n",
        "    weight_decay=0.03,                     # é©ç•¶æ­£å‰‡åŒ– 0.03\n",
        "    # lr_scheduler_type=\"linear\",  # ç·šæ€§è¡°æ¸›\n",
        "    # per_device_train_batch_size=8,        # æ‰¹é‡å¤§ä¸€é»ä¹Ÿæœ‰åŠ©ç©©å®š\n",
        "    per_device_eval_batch_size=64,\n",
        "    # load_best_model_at_end=True,           # âš ï¸ æ­é… EarlyStopping æ™‚å¾ˆé‡è¦\n",
        "    # metric_for_best_model=\"eval_loss\",     # æ ¹æ“š loss é¸æœ€ä½³æ¨¡å‹\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "task2_path = answer_val_data_path_txt\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=data_train,\n",
        "    eval_dataset=data_test,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[CharBasedEvaluationCallback(task2_path, tokenizer)]\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
